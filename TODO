TODO:
  tilted distributions
  Wishart and Inverse Wishart Mixin.
  event_shape for Dirichlet + MvNormal Mixins


Tilted distributions:
  tr.sample("a", TiltedNormal(0, 1))
  Problem: same line of code is run for both P and Q, returning the same distribution object.
  That single distribution object must contain P and Q, which is kind-of painful.
  Key trick: we only need Q in a very specific bit of tracing code (the bit where we use the prior as an approximate posterior in Trace P).
  So we treat the distribution everywhere as P, except for in that little bit of code.
  We include a link back to the tilting...

GLMs:
  Only use cannonical link (otherwise you need to map from mean to natural params, and differentiate back through that mapping).
  Only really makes sense for Linear and Logistic Regression (and maybe Poisson).
  Combine inputs (which the dist holds on to) (which the dist holds on to) with gradients of beta.
   (which the dist holds on to)

Compute objective
  KL(Q reweighted towards P| Q)

Cleanup ML approximate posteriors
  Extra features:
    Mixture model
    Linear regression 
    GLM regression
    Modify natural parameters on prior
  MLModules for P too?
    Model sets a flag indicating we're in P and therefore we should use + (not -) extra_log_factor.
    Asserts that no plates when you're using it as P.
    What happens if an MLModule turns up in both P and Q?
  Test conv2mean by sampling

MCMC:
  ...

Stein:
  Do Stein separately along each K.
  We compute the marginal probability, summing over all the other particles.
  These look alot like the marginal importance weights.
  There is a key difference thought: marginal importance weights are normalized, which means there is implicitly a normalizer that depends on the chosen particles.
  May need to remove this normalizer by adding log P_tmc back in.

Papers:
  Stats paper.
  ML paper.
  MCMC paper.

torch.no_grad / detach e.g. for sampling
Naming:
  Kdim etc.

Error messages:
  plate names and var names don't overlap!
  check all dimensions that are either plates or Ks
  Error if model.P has any QModules
  

Tests:
  Predictive posteriors
  Q-Q plots: P(z) = \int P(x) P(z|x) 
    Repeatedly sample data, x, from the prior.
    Sample P(z|x).
    Specifically, compare moments.
    Can be done very quickly with importance sampling.
  Timeseries: 
    Moments from rolled vs unrolled models are the same.
    GP-distributed timeseries.
  Unit tests for the reduction utilities.

Multiple iterations of importance weighting 

torchdims
  Cleanup `==` vs `is` for dimension comparison.
  Check whether there are some more torchdim conventions which could clean up the code.

Needing to convert log-probs to float64 seems flakey.
  Ultimately, the problem is that use einsum, rather than log-sum-exp for each reduction.
  Is there something better we can do?

multisample=False.  Ideally, we would
  die when we try RWS
  do something sensible in importance sampling/weighting with multisample=False

Do something with covariates?
  They currently come in as data, which isn't correct.
  Really, they should come in as a `covariates` dict (like the data dict).
  This would come in in `Model` and all the other methods `VI` etc.
  Allows error checking (e.g. we can check that all the data have a corresponding log-likelihood).

Error checking for minibatching.
  Currently, data enters through `Model` or through e.g. `vi`.
  Data entering through `vi` can be minibatched.
  But minibatched data has restrictions:
    We can't (at least currently) learn datapoint specific parameters.
    We can't have shared K-dimensions with minibatched data.
  And we don't currently check these restrictions.

Masks / nested tensors for different numbers of observations in a plate...

Reduce memory consumption:
  Tips in docs:
    Group everything in a plate.
    Make sure your heirarchy doesn't skip a plate.
  Memory checkpointing (maybe around reduce_Ks)?

Memory diagnostics:
  Method to print size of log_prob for all tensors, in TraceP
  device='meta' seems to work with torchdims

Mixture models
  Most of the obvious approaches won't work too well due to symmetry (e.g. each datapoint has an equal chance of being assigned to each cluster).
  The right approach is to define a prior + approximate posterior over partitions.
  Approximate posterior should be written in terms of "affinities" (datapoint 4 wants to be with datapoint 3 but not datapoints 1 or 2).
  We can use the partition to define a dynamic plate (as we have independence across clusters).

What is the right ESS?

Masked Tensors / Nested Tensors with torchdims?

Finally solve memory problems?
  Probably you only want to split over a plate.  This is nice because:
    It is easy for the user to reason about (they know about plates but not Ks).
    We only need to implement the sum-reduction, rather than the einsum for Ks
  Implementation:
    do the forward pass in a for-loop
    use torch.utils.checkpoint in the loop to avoid saving memory at each iteration.
    may need to implement a torch.autograd.Function to avoid _ever_ instansiating/saving the full tensor?
    Implementation should be easy for plates, but not timeseries
    Can't split twice as I assume we can't nest torch.utils.checkpoint.

  BatchTensor:
    Manually specify some tensors as BatchTensor, and give the dimension(s) along which we batch.
    When we do an op on a BatchTensor, we get a LazyTensor.

  LazyTensor:
    Just records input args + func.
    LazyTensor knows all the batched torchdims.
    Once we reduce over the last batched torchdim, we actually perform the computation.
    
  Check whether meta tensors work with torchdim
