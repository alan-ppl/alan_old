Tests:
  Predictive posteriors
  Q-Q plots: P(z) = \int P(x) P(z|x) 
    Repeatedly sample data, x, from the prior.
    Sample P(z|x).
    Specifically, compare moments.
    Can be done very quickly with importance sampling.
  Timeseries: 
    Moments from rolled vs unrolled models are the same.
    GP-distributed timeseries.

Shift logic for combining TimeseriesLogP into TimeseriesLogP

torchdims
  Cleanup `==` vs `is` for dimension comparison.
  Check whether there are some more torchdim conventions which could clean up the code.

Needing to convert log-probs to float64 seems flakey.
  Ultimately, the problem is that use einsum, rather than log-sum-exp for each reduction.
  Is there something better we can do?

multisample=False.  Ideally, we would
  die when we try RWS
  do something sensible in importance sampling/weighting with multisample=False

Do something with covariates?
  They currently come in as data, which isn't correct.
  Really, they should come in as a `covariates` dict (like the data dict).
  This would come in in `Model` and all the other methods `VI` etc.
  Allows error checking (e.g. we can check that all the data have a corresponding log-likelihood).

Error checking for minibatching.
  Currently, data enters through `Model` or through e.g. `vi`.
  Data entering through `vi` can be minibatched.
  But minibatched data has restrictions:
    We can't (at least currently) learn datapoint specific parameters.
    We can't have shared K-dimensions with minibatched data.
  And we don't currently check these restrictions.

Masks / nested tensors for different numbers of observations in a plate...

Reduce memory consumption:
  Tips in docs:
    Group everything in a plate.
    Make sure your heirarchy doesn't skip a plate.
  Memory checkpointing (maybe around reduce_Ks)?

Memory diagnostics:
  Method to print size of log_prob for all tensors, in TraceP
  device='meta' seems to work with torchdims


Natural gradient updates
  Beta + inverse gamma

  Construct an approximate posterior from:
    A bunch of samples from the prior to give:
      sizes
      initialization (also requires multiple samples from prior)
    A few extra bits and pieces (distribution, optionally init)

  If we do multisamples, we should be able to update using wake-phase Q update.
    Should be equivalent to importance weighting first.

  Natural gradient updates should also work with the elbo if we multi_sample = False?

  Have a bunch of "base" classes representing e.g. a Gaussian approximate posterior.
  Natural gradients:
    Question: How to deal with non-negativity?
    Advantage: Works with multi_sample=False
    Implementation: classes with parameters with requires_grad=True, but they aren't parameters?
  Moment updates:
    The classes are parameterised by their mean parameters.
    Updated the mean parameters towards the values from the last importance-weighted posterior sample.
    Only works with multi_sample=True
    Works with sufficient statistics (as after the update, its like computing the expected sufficient statistics of a mixture).
    Implementation: Only needs methods to convert sufficient statistics to conventional params

Mixture models
  Most of the obvious approaches won't work too well due to symmetry (e.g. each datapoint has an equal chance of being assigned to each cluster).
  The right approach is to define a prior + approximate posterior over partitions.
  Approximate posterior should be written in terms of "affinities" (datapoint 4 wants to be with datapoint 3 but not datapoints 1 or 2).
  We can use the partition to define a dynamic plate (as we have independence across clusters).

Finally solve memory problems?
  Probably you only want to split over a plate.  This is nice because:
    It is easy for the user to reason about (they know about plates but not Ks).
    We only need to implement the sum-reduction, rather than the einsum for Ks
  Implementation:
    do the forward pass in a for-loop
    use torch.utils.checkpoint in the loop to avoid saving memory at each iteration.
    may need to implement a torch.autograd.Function to avoid _ever_ instansiating/saving the full tensor?
    Implementation should be easy for plates, but not timeseries
    Can't split twice as I assume we can't nest torch.utils.checkpoint.

  BatchTensor:
    Manually specify some tensors as BatchTensor, and give the dimension(s) along which we batch.
    When we do an op on a BatchTensor, we get a LazyTensor.

  LazyTensor:
    Just records input args + func.
    LazyTensor knows all the batched torchdims.
    Once we reduce over the last batched torchdim, we actually perform the computation.
    
  Check whether meta tensors work with torchdim
