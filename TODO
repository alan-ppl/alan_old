Papers:
  ML:
    Proof of validity of RWS and VI + superiority over TMC (requires TMC).
  Stats:
    log-derivatives for computing marginals + posterior samples
  Fast proposals:
    fast ML learning of factorised approximate posteriors
    tilted / pseudo-likelihood based approximate posteriors + a fast learning mechanism
  MCMC:
    6)

Tech TODOs:
  TMC!  Need grouping, but not timeseries.
  Wishart and Inverse Wishart Mixin.
  event_shape for Dirichlet + MvNormal Mixins
  Objective: KL(Q reweighted towards P| Q)
  Cleanup objectives:
    Objective for just learning Q.
    Objective for computing moments (detach logps and logqs, but not extra_log_factors).
  Test conv2mean by sampling
  GLMs
    Only use cannonical link (otherwise you need to map from mean to natural params, and differentiate back through that mapping).
    Only really makes sense for Linear and Logistic Regression (and maybe Poisson).
    Combine inputs (which the dist holds on to) (which the dist holds on to) with gradients of beta.
     (which the dist holds on to)
  Good story about mixture models.
  Masking.
  GPs
  Multiple iterations of importance weighting 
  Memory problems!
  

Error messages:
  plate names and var names don't overlap!
  check all dimensions that are either plates or Ks
  Error if model.P has any QModules
  

Tests:
  Predictive posteriors
  Q-Q plots: P(z) = \int P(x) P(z|x) 
    Repeatedly sample data, x, from the prior.
    Sample P(z|x).
    Specifically, compare moments.
    Can be done very quickly with importance sampling.
  Timeseries: 
    Moments from rolled vs unrolled models are the same.
    GP-distributed timeseries.
  Unit tests for the reduction utilities.


torchdims
  Cleanup `==` vs `is` for dimension comparison.
  Check whether there are some more torchdim conventions which could clean up the code.

Needing to convert log-probs to float64 seems flakey.
  Ultimately, the problem is that use einsum, rather than log-sum-exp for each reduction.
  Is there something better we can do?

multisample=False.  Ideally, we would
  die when we try RWS
  do something sensible in importance sampling/weighting with multisample=False

Do something with covariates?
  They currently come in as data, which isn't correct.
  Really, they should come in as a `covariates` dict (like the data dict).
  This would come in in `Model` and all the other methods `VI` etc.
  Allows error checking (e.g. we can check that all the data have a corresponding log-likelihood).

Error checking for minibatching.
  Currently, data enters through `Model` or through e.g. `vi`.
  Data entering through `vi` can be minibatched.
  But minibatched data has restrictions:
    We can't (at least currently) learn datapoint specific parameters.
    We can't have shared K-dimensions with minibatched data.
  And we don't currently check these restrictions.

Masks / nested tensors for different numbers of observations in a plate...

Reduce memory consumption:
  Tips in docs:
    Group everything in a plate.
    Make sure your heirarchy doesn't skip a plate.
  Memory checkpointing (maybe around reduce_Ks)?

Memory diagnostics:
  Method to print size of log_prob for all tensors, in TraceP
  device='meta' seems to work with torchdims

Mixture models
  Most of the obvious approaches won't work too well due to symmetry (e.g. each datapoint has an equal chance of being assigned to each cluster).
  The right approach is to define a prior + approximate posterior over partitions.
  Approximate posterior should be written in terms of "affinities" (datapoint 4 wants to be with datapoint 3 but not datapoints 1 or 2).
  We can use the partition to define a dynamic plate (as we have independence across clusters).

What is the right ESS?

Masked Tensors / Nested Tensors with torchdims?

Finally solve memory problems?
  Probably you only want to split over a plate.  This is nice because:
    It is easy for the user to reason about (they know about plates but not Ks).
    We only need to implement the sum-reduction, rather than the einsum for Ks
  Implementation:
    do the forward pass in a for-loop
    use torch.utils.checkpoint in the loop to avoid saving memory at each iteration.
    may need to implement a torch.autograd.Function to avoid _ever_ instansiating/saving the full tensor?
    Implementation should be easy for plates, but not timeseries
    Can't split twice as I assume we can't nest torch.utils.checkpoint.

  BatchTensor:
    Manually specify some tensors as BatchTensor, and give the dimension(s) along which we batch.
    When we do an op on a BatchTensor, we get a LazyTensor.

  LazyTensor:
    Just records input args + func.
    LazyTensor knows all the batched torchdims.
    Once we reduce over the last batched torchdim, we actually perform the computation.
    
  Check whether meta tensors work with torchdim
