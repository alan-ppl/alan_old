multisample = False
  Warning that lots of things break if you use multisample=False.
  Do something sensible in importance sampling/weighting with multisample=False

We might want an approximate posterior that goes up the plate heirarchy.
  In that case, we probably want an approximate posterior which allows us to e.g. average over plates.
  We could do this averaging through trace (e.g. `tr.mean(rv, "plate_1")`)
  This API is a bit strange, but means that we can allow averaging in Q but not P.

Do something with covariates?
  They currently come in as data, which isn't correct.
  Really, they should come in as a `covariates` dict (like the data dict).
  This would come in in `Model` and all the other methods `VI` etc.
  Allows error checking (e.g. we can check that all the data have a corresponding log-likelihood).

Error checking for minibatching.
  Currently, data enters through `Model` or through e.g. `vi`.
  Data entering through `vi` can be minibatched.
  But minibatched data has restrictions:
    We can't (at least currently) learn datapoint specific parameters.
    We can't have shared K-dimensions with minibatched data.
  And we don't currently check these restrictions.

Masks / nested tensors for different numbers of observations in a plate...

Reduce memory consumption:
  Tips in docs:
    Group everything in a plate.
    Make sure you're heirarchy doesn't skip a plate.
  Memory checkpointing (maybe around reduce_Ks)?

Memory diagnostics:
  Method to print size of log_prob for all tensors.
  Maybe use device="meta" so that we don't run out of memory?

Tests:
  Importance sampling and importance weighting give the same moments.
  Timeseries: 
    ELBO is the same if we unroll; test by handwriting equivalent traces. 
    Moments work.

Timeseries importance sampling
  Abstract out some part of the sampling process, so it can be re-used for plates and timeseries.
       

Fast / natural gradient updates

Sum out discrete variables:
  Sum them out in P;
  Don't appear in Q.

Check that when doing importance sampling we can specify a subset of the plate sizes.

Finally solve memory problems?
  Probably you only want to split over a plate.  This is nice because:
    It is easy for the user to reason about (they know about plates but not Ks).
    We only need to implement the sum-reduction, rather than the einsum for Ks
  Implementation:
    do the forward pass in a for-loop
    use torch.utils.checkpoint in the loop to avoid saving memory at each iteration.
    may need to implement a torch.autograd.Function to avoid _ever_ instansiating/saving the full tensor?
    Implementation should be easy for plates, but not timeseries
    Can't split twice as I assume we can't nest torch.utils.checkpoint.

  BatchTensor:
    Manually specify some tensors as BatchTensor, and give the dimension(s) along which we batch.
    When we do an op on a BatchTensor, we get a LazyTensor.

  LazyTensor:
    Just records input args + func.
    LazyTensor knows all the batched torchdims.
    Once we reduce over the last batched torchdim, we actually perform the computation.
    
  Check whether meta tensors work with torchdim
