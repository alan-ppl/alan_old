Tests:
  Timeseries: 
    Moments from rolled vs unrolled models are the same.
    GP-distributed timeseries.
  Plates:
    ??
  Q-Q plots:
    P(z) = \int P(x) P(z|x) 
    i.e. distribution of latents from prior is the same as:
      repeatedly
      sampling data, x, from the prior
      sampling latents, z
  Importance sampling and importance weighting give the same moments.
  Natural gradients:
    Test conv -> mean by sampling.
  Conjugate priors.

Discrete variables:
  Key difference with discrete variables is that we _sum_ we don't _average_ over the probabilities
    (because there is a P(discrete variable) term, which would be 1/N if the prior is discrete).
  To prep for discrete variables, we shift the subtracting of the log K terms out of the tensor product.
  Instead, subtract log K from logP in TraceP, whenever we encounter a new K.

torchdims
  Cleanup `==` vs `is` for dimension comparison.
  Check whether there are some more torchdim conventions which could clean up the code.

Needing to convert log-probs to float64 seems flakey.
  Ultimately, the problem is that use einsum, rather than log-sum-exp for each reduction.
  Is there something better we can do?

multisample=False.  Ideally, we would
  die when we try RWS
  do something sensible in importance sampling/weighting with multisample=False

Do something with covariates?
  They currently come in as data, which isn't correct.
  Really, they should come in as a `covariates` dict (like the data dict).
  This would come in in `Model` and all the other methods `VI` etc.
  Allows error checking (e.g. we can check that all the data have a corresponding log-likelihood).

Error checking for minibatching.
  Currently, data enters through `Model` or through e.g. `vi`.
  Data entering through `vi` can be minibatched.
  But minibatched data has restrictions:
    We can't (at least currently) learn datapoint specific parameters.
    We can't have shared K-dimensions with minibatched data.
  And we don't currently check these restrictions.

Masks / nested tensors for different numbers of observations in a plate...

Reduce memory consumption:
  Tips in docs:
    Group everything in a plate.
    Make sure your heirarchy doesn't skip a plate.
  Memory checkpointing (maybe around reduce_Ks)?

Memory diagnostics:
  Method to print size of log_prob for all tensors, in TraceP
  device='meta' seems to work with torchdims


Natural gradient updates
  Beta + inverse gamma

  Construct an approximate posterior from:
    A bunch of samples from the prior to give:
      sizes
      initialization (also requires multiple samples from prior)
    A few extra bits and pieces (distribution, optionally init)

  If we do multisamples, we should be able to update using wake-phase Q update.
    Should be equivalent to importance weighting first.

  Natural gradient updates should also work with the elbo if we multi_sample = False?

  Have a bunch of "base" classes representing e.g. a Gaussian approximate posterior.
  Natural gradients:
    Question: How to deal with non-negativity?
    Advantage: Works with multi_sample=False
    Implementation: classes with parameters with requires_grad=True, but they aren't parameters?
  Moment updates:
    The classes are parameterised by their mean parameters.
    Updated the mean parameters towards the values from the last importance-weighted posterior sample.
    Only works with multi_sample=True
    Works with sufficient statistics (as after the update, its like computing the expected sufficient statistics of a mixture).
    Implementation: Only needs methods to convert sufficient statistics to conventional params


    
    
  

Sum out discrete variables:
  Not necessary as we have RWS.
  Most obvious approach is to sum them out in P, so they don't appear in Q.
  Implemented as K-dimensions with different sizes.

Check that when doing importance sampling we can specify a subset of the plate sizes.

Finally solve memory problems?
  Probably you only want to split over a plate.  This is nice because:
    It is easy for the user to reason about (they know about plates but not Ks).
    We only need to implement the sum-reduction, rather than the einsum for Ks
  Implementation:
    do the forward pass in a for-loop
    use torch.utils.checkpoint in the loop to avoid saving memory at each iteration.
    may need to implement a torch.autograd.Function to avoid _ever_ instansiating/saving the full tensor?
    Implementation should be easy for plates, but not timeseries
    Can't split twice as I assume we can't nest torch.utils.checkpoint.

  BatchTensor:
    Manually specify some tensors as BatchTensor, and give the dimension(s) along which we batch.
    When we do an op on a BatchTensor, we get a LazyTensor.

  LazyTensor:
    Just records input args + func.
    LazyTensor knows all the batched torchdims.
    Once we reduce over the last batched torchdim, we actually perform the computation.
    
  Check whether meta tensors work with torchdim
