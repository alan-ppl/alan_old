multisample=False:
  Die when we try RWS.
  Do something sensible in importance sampling/weighting with multisample=False

Do something with covariates?
  They currently come in as data, which isn't correct.
  Really, they should come in as a `covariates` dict (like the data dict).
  This would come in in `Model` and all the other methods `VI` etc.
  Allows error checking (e.g. we can check that all the data have a corresponding log-likelihood).

Error checking for minibatching.
  Currently, data enters through `Model` or through e.g. `vi`.
  Data entering through `vi` can be minibatched.
  But minibatched data has restrictions:
    We can't (at least currently) learn datapoint specific parameters.
    We can't have shared K-dimensions with minibatched data.
  And we don't currently check these restrictions.

Masks / nested tensors for different numbers of observations in a plate...

Reduce memory consumption:
  Tips in docs:
    Group everything in a plate.
    Make sure you're heirarchy doesn't skip a plate.
  Memory checkpointing (maybe around reduce_Ks)?

Memory diagnostics:
  Method to print size of log_prob for all tensors, in TraceP

Tests:
  Importance sampling and importance weighting give the same moments.
  Timeseries: 
    ELBO is the same if we unroll; test by handwriting equivalent traces. 
    Moments work.

Timeseries importance sampling
  Abstract out some part of the sampling process, so it can be re-used for plates and timeseries.
       
Fast / natural gradient updates

Sum out discrete variables:
  Not necessary as we have RWS.
  Most obvious approach is to sum them out in P, so they don't appear in Q.
  Implemented as K-dimensions with different sizes.

Check that when doing importance sampling we can specify a subset of the plate sizes.

Finally solve memory problems?
  Probably you only want to split over a plate.  This is nice because:
    It is easy for the user to reason about (they know about plates but not Ks).
    We only need to implement the sum-reduction, rather than the einsum for Ks
  Implementation:
    do the forward pass in a for-loop
    use torch.utils.checkpoint in the loop to avoid saving memory at each iteration.
    may need to implement a torch.autograd.Function to avoid _ever_ instansiating/saving the full tensor?
    Implementation should be easy for plates, but not timeseries
    Can't split twice as I assume we can't nest torch.utils.checkpoint.

  BatchTensor:
    Manually specify some tensors as BatchTensor, and give the dimension(s) along which we batch.
    When we do an op on a BatchTensor, we get a LazyTensor.

  LazyTensor:
    Just records input args + func.
    LazyTensor knows all the batched torchdims.
    Once we reduce over the last batched torchdim, we actually perform the computation.
    
  Check whether meta tensors work with torchdim
