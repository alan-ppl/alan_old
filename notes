Q returns:
  dict[name -> logQ["plate_1", "plate_2", "K"]
  dict[name -> sample(tensor with one "K" dim)]

Q:
  sample["a"] = Normal(..., ...)
  logp["a"] = 

We transform
  dict[name -> sample(tensor with one "K" dim)]
to
  dict[name -> sample(tensor with "K_name" dims)]


P:
  takes:
    dict[name -> sample(tensor with "K_name" dims)]
  and returns:
    dict[name -> logP(tensor with "K_name" dims)]
  P doesn't need to know about tensor dims, because they're already in the input samples.
  Just needs to make sure it only manipulates rightmost "positional" dimensions.


def P(sample):
    logp = {}
    logp["a"] = Normal(..., ...).log_prob(sample["a"])
    logp["b"]
    return logp

  

Papers:
single standard proposal in each dimension followed by Gibbs sampling for k \in {0, 1}
  good balance of simplicity and efficiency
  with symmetric MHMC proposal, don't even need to compute proposal probability!
  extension: multiple samples in each dimension (PNAS 2014)
use VI objective and recognition model to estimate good proposal bandwidth
  reparameterise for HMC/Langevin
define proposals/variational posteriors that capture strong prior correlation by conditioning
reparameterised tensorised Langevin/HMC
  need full joint rejection
  initially use non-indexed samples IID N(0, 1)
  later add attraction between all samples, such that marginally, particles still generated N(0, 1) + likelihood
