{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "PRIOR_MEAN = 0.9\n",
    "PRIOR_VAR = 1.0\n",
    "GROUND_SIGMA = 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical solution for lognormal\n",
    "def log_norm(x, mu, std):    \n",
    "    var = std**2\n",
    "    norm_constant = -0.5 * t.log(2*np.pi*var)\n",
    "    sqerror = (x - mu)**2\n",
    "    prec = 1/var\n",
    "    \n",
    "    return norm_constant - (0.5 * prec * sqerror)\n",
    "\n",
    "\n",
    "class Elbo(nn.Module):\n",
    "    def __init__(self, n=100):\n",
    "        super(Elbo, self).__init__()\n",
    "        \n",
    "        self.batch_size = n # latent samples per step\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        # adaptive variational params\n",
    "        self.q_mean = nn.Parameter(t.randn(1,1), requires_grad=True)\n",
    "        self.q_sigma = nn.Parameter(t.randn(1,1), requires_grad=True)\n",
    "        self.prior_m = nn.Parameter(t.randn(1,1), requires_grad=False)\n",
    "        self.prior_s = nn.Parameter(t.randn(1,1), requires_grad=False)\n",
    "        self.likelihood_s = nn.Parameter(t.FloatTensor((1)), requires_grad=False)\n",
    "        \n",
    "        #Set the prior and likelihood moments.\n",
    "        self.prior_s.data.fill_(PRIOR_VAR)\n",
    "        self.prior_m.data.fill_(PRIOR_MEAN)\n",
    "        self.likelihood_s.data.fill_(GROUND_SIGMA)\n",
    "     \n",
    "        \n",
    "    def generate_rand(self):\n",
    "        return np.random.normal(size=(self.batch_size,1))\n",
    "    \n",
    "    \n",
    "    def get_mean(self) :\n",
    "        return self.q_mean.data.numpy()\n",
    "    \n",
    "    \n",
    "    def get_var(self) :\n",
    "        torch_var = self.softplus(self.q_sigma).data**2\n",
    "        return torch_var.numpy()\n",
    "    \n",
    "    \n",
    "    def reparam(self, eps):\n",
    "        eps = nn.Parameter(t.FloatTensor(eps))\n",
    "        \n",
    "        return eps.mul(self.softplus(self.q_sigma)) \\\n",
    "                .add(self.q_mean)\n",
    "    \n",
    "    \n",
    "    def log_prob(self, y, x) :\n",
    "        return log_norm(y, x, self.likelihood_s)\n",
    "    \n",
    "    \n",
    "    def compute_elbo(self, x, y):\n",
    "        eps = self.generate_rand()\n",
    "        z = self.reparam(eps)\n",
    "        \n",
    "        q_likelihood = t.mean(log_norm(z, self.q_mean, self.softplus(self.q_sigma)))\n",
    "        prior = t.mean(log_norm(z, self.prior_m, self.prior_s))\n",
    "        \n",
    "        xzt = x * z.transpose(0,1)\n",
    "        sum_log_prob = t.sum(self.log_prob(y, xzt), 0)\n",
    "        likelihood = t.mean(sum_log_prob)\n",
    "        \n",
    "        kl_div_mc = q_likelihood - prior\n",
    "        loss = likelihood - kl_div_mc\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "w = 3.2\n",
    "\n",
    "X = np.random.uniform(low=-50, high=50, size=(N, 1))\n",
    "Y = w *X + np.random.normal(size=(N, 1), scale=GROUND_SIGMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4a894ab571c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \"\"\"\n",
      "\u001b[0;32m<ipython-input-5-4a894ab571c4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(X, Y, ep, eta)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3000\n",
    "\n",
    "def run(X, Y, ep=5000, eta=0.2) :\n",
    "    q = Elbo()\n",
    "    optimiser = t.optim.Adam(q.parameters(), lr=eta)\n",
    "    x = Variable(t.Tensor(X), requires_grad=False) \n",
    "    y = Variable(t.Tensor(Y), requires_grad=False)\n",
    "\n",
    "    optimise(q, x, y, optimiser, ep)\n",
    "    \n",
    "    return q\n",
    "\n",
    "\n",
    "def optimise(q, x, y, optimiser, ep, verbose=False) :\n",
    "    for i in range(ep):\n",
    "        loss = - q.compute_elbo(x, y)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimiser.step()\n",
    "\n",
    "        if verbose :\n",
    "            if i % 500 == 0:\n",
    "                print(q.get_mean(), q.get_var())\n",
    "\n",
    "\n",
    "\n",
    "q = run(X, Y, ep=EPOCHS)\n",
    "\n",
    "\"\"\"\n",
    "means = []\n",
    "\n",
    "for i in range(10) :\n",
    "    q = run(X, Y, ep=EPOCHS)\n",
    "    means.append(q.get_mean())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_posterior_var(var, X) :\n",
    "    scaled_prec = (1/var**2) * X.T @ X +1\n",
    "    \n",
    "    return scaled_prec**-1\n",
    "\n",
    "\n",
    "def analytical_posterior_mean(prior_mean, var, X, Y) :\n",
    "    scaled_cov = (1/var**2) * X.T @ Y\n",
    "    post_var = analytical_posterior_var(var, X)\n",
    "    \n",
    "    return post_var * (prior_mean + scaled_cov)\n",
    "\n",
    "\n",
    "\n",
    "TRUE_POST_MEAN = analytical_posterior_mean(GROUND_PRIOR_MEAN, GROUND_PRIOR_VAR, X, Y)\n",
    "TRUE_POST_VAR = analytical_posterior_var(GROUND_PRIOR_VAR, X)\n",
    "\n",
    "q.get_mean() - TRUE_POST_MEAN, \\\n",
    "q.get_var() - TRUE_POST_VAR\n",
    "\n",
    "\n",
    "#np.mean(means) - TRUE_POST_MEAN "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
