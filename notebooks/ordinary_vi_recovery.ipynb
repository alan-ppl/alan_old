{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "GROUND_PRIOR_MEAN = 0.9\n",
    "GROUND_PRIOR_VAR = 1.0\n",
    "GROUND_SIGMA = 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_norm(x, mu, std):    \n",
    "    norm_constant = -0.5 * t.log(2*np.pi*std**2)\n",
    "    \n",
    "    return norm_constant - (0.5 * (1/(std**2))* (x-mu)**2)\n",
    "\n",
    "\n",
    "class Elbo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Elbo, self).__init__()\n",
    "        self.n_latent = 100 # Number of latent samples\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        # adaptive variational params\n",
    "        self.qm = nn.Parameter(t.randn(1,1), requires_grad=True)\n",
    "        self.qs = nn.Parameter(t.randn(1,1), requires_grad=True)\n",
    "        \n",
    "        #create holders for prior mean and std, and likelihood std.\n",
    "        self.prior_m = Variable(t.randn(1,1), requires_grad=False)\n",
    "        #self.prior_m = nn.Parameter(t.randn(1,1), requires_grad=False)\n",
    "        self.prior_s = Variable(t.randn(1,1), requires_grad=False)\n",
    "        # self.prior_s = nn.Parameter(t.randn(1,1), requires_grad=False)\n",
    "        self.likelihood_s = Variable(t.FloatTensor((1)), requires_grad=False)\n",
    "        # self.likelihood_s = nn.Parameter(t.FloatTensor((1)), requires_grad=False)\n",
    "        \n",
    "        #Set the prior and likelihood moments.\n",
    "        self.prior_s.data.fill_(GROUND_PRIOR_VAR)\n",
    "        self.prior_m.data.fill_(GROUND_PRIOR_MEAN)\n",
    "        self.likelihood_s.data.fill_(GROUND_SIGMA)\n",
    "     \n",
    "        \n",
    "    def generate_rand(self):\n",
    "        return np.random.normal(size=(self.n_latent,1))\n",
    "    \n",
    "    \n",
    "    def get_mean(self) :\n",
    "        return self.qm.data.numpy()\n",
    "    \n",
    "    \n",
    "    def get_var(self) :\n",
    "        torch_var = self.softplus(self.qs).data**2\n",
    "        return torch_var.numpy()\n",
    "    \n",
    "    \n",
    "    def reparam(self, eps):\n",
    "        eps = Variable(t.FloatTensor(eps))\n",
    "        # eps = nn.Parameter(t.FloatTensor(eps))\n",
    "        \n",
    "        return eps.mul(self.softplus(self.qs)) \\\n",
    "                .add(self.qm)\n",
    "    \n",
    "    \n",
    "    def compute_elbo(self, x, y):\n",
    "        eps = self.generate_rand()\n",
    "        z = self.reparam(eps)\n",
    "        \n",
    "        q_likelihood = t.mean(log_norm(z, self.qm, self.softplus(self.qs)))\n",
    "        prior = t.mean(log_norm(z, self.prior_m, self.prior_s))\n",
    "        \n",
    "        xz = x * z.transpose(0,1)\n",
    "        sum_log_prob = t.sum(log_norm(y, xz, self.likelihood_s), 0)\n",
    "        likelihood = t.mean(sum_log_prob)\n",
    "        \n",
    "        kl_div_mc = q_likelihood - prior\n",
    "        loss = likelihood - kl_div_mc\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "w = 3.2\n",
    "\n",
    "X = np.random.uniform(low=-50, high=50, size=(N, 1))\n",
    "Y = w *X + np.random.normal(size=(N, 1), scale=GROUND_SIGMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77676654]] [[0.15058462]]\n",
      "[[3.1996546]] [[0.00028702]]\n",
      "[[3.1990616]] [[0.00018747]]\n",
      "[[3.1990438]] [[0.00016577]]\n",
      "[[3.2004526]] [[0.0001654]]\n",
      "[[3.2011387]] [[0.00016327]]\n",
      "[[3.1997194]] [[0.0001637]]\n",
      "[[3.2006679]] [[0.00015809]]\n",
      "[[3.2003458]] [[0.00015999]]\n",
      "[[3.2033272]] [[0.0001635]]\n",
      "[[3.1951232]] [[0.00016368]]\n",
      "[[3.20158]] [[0.00016144]]\n",
      "[[3.1981337]] [[0.00016061]]\n",
      "[[3.201482]] [[0.00016215]]\n",
      "[[3.1979587]] [[0.00016075]]\n"
     ]
    }
   ],
   "source": [
    "q = Elbo()\n",
    "optimizer = t.optim.Adam(q.parameters(), lr=0.2)\n",
    "x = Variable(t.Tensor(X), requires_grad=False) \n",
    "y = Variable(t.Tensor(Y), requires_grad=False)\n",
    "\n",
    "for i in range(3501):\n",
    "    loss = - q.compute_elbo(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 250 ==0:\n",
    "        print(q.get_mean(), q.get_var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00242217]]), array([[0.00015538]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analytical_posterior_var(var, X) :\n",
    "    scaled_prec = (1/var**2) * X.T @ X +1\n",
    "    \n",
    "    return scaled_prec**-1\n",
    "\n",
    "\n",
    "def analytical_posterior_mean(prior_mean, var, X, Y) :\n",
    "    scaled_cov = (1/var**2) * X.T @ Y\n",
    "    post_var = analytical_posterior_var(var, X)\n",
    "    \n",
    "    return post_var * (prior_mean + scaled_cov)\n",
    "\n",
    "\n",
    "\n",
    "TRUE_POST_MEAN = analytical_posterior_mean(GROUND_PRIOR_MEAN, GROUND_PRIOR_VAR, X, Y)\n",
    "TRUE_POST_VAR = analytical_posterior_var(GROUND_PRIOR_VAR, X)\n",
    "\n",
    "q.get_mean() - TRUE_POST_MEAN, \\\n",
    "q.get_var() - TRUE_POST_VAR\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
