{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMC by hand, in raw Pytorch \n",
    "\n",
    "e.g. Conjugate Gaussian chain\n",
    "\n",
    "no `trace` or combine\n",
    "\n",
    "Should be possible to do a chain model (\"just a list of matrices\")\n",
    " \n",
    " `a = N(0, s)`\n",
    " \n",
    " `b = N(a, s)`\n",
    " \n",
    " `c = N(b, s)`\n",
    " \n",
    " i.e. one known variable $c$: the data, at the end of the chain\n",
    "\n",
    "\n",
    "* TMC module \n",
    "  * P module\n",
    "    * ParamNormal modules\n",
    "  * Q module\n",
    "    * ParamNormal modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.distributions import MultivariateNormal as MVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent a normal as two adaptive params and an eval method\n",
    "# from https://github.com/anonymous-78913/tmc-anon/blob/master/param/lvm.py\n",
    "class ParamNormal(nn.Module):\n",
    "    def __init__(self, shape, mean=0, scale=1.):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Parameter(t.ones(size=shape) * mean)\n",
    "        self.log_scale = nn.Parameter(t.ones(shape) * math.log(scale))\n",
    "\n",
    "        \n",
    "    def forward(self):\n",
    "        return Normal(self.loc, self.log_scale.exp())\n",
    "\n",
    "\n",
    "class LinearNormal(nn.Module):\n",
    "    def __init__(self, shape=t.Size([]), scale=1.):\n",
    "        super().__init__()\n",
    "        self.log_scale = nn.Parameter(t.ones(shape) * math.log(scale))\n",
    "\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        return Normal(input_, self.log_scale.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardcode a basic chain a -> b -> x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generative_model\n",
    "# TODO: let s vary\n",
    "class ChainP(nn.Module):\n",
    "    def __init__(self, sigma, mean=0):\n",
    "        super().__init__()\n",
    "        self.Pa = ParamNormal((), mean=mean, scale=sigma)\n",
    "        self.Pb = LinearNormal((), scale=sigma)\n",
    "        self.Px = LinearNormal((), scale=sigma)\n",
    "\n",
    "        \n",
    "    def sample(self, N):\n",
    "        a = self.Pa().rsample()\n",
    "        b = self.Pb(a).rsample(sample_shape=t.Size([N]))\n",
    "        x = self.Px(b).rsample()\n",
    "        \n",
    "        return x, a.unsqueeze(-1), b\n",
    "\n",
    "    \n",
    "    def log_prob(self, samples):\n",
    "        x, a, b = samples\n",
    "        log_Pa = self.Pa().log_prob(a)\n",
    "        log_Pb = self.Pb(a).log_prob(b)\n",
    "        log_Px = self.Px(b).log_prob(x)\n",
    "        \n",
    "        return log_Pa.sum(-1) \\\n",
    "                + log_Pb.sum(-1) \\\n",
    "                + log_Px.sum(-1)\n",
    "\n",
    "\n",
    "# TODO: isotropic?\n",
    "class ChainQ(nn.Module):\n",
    "    def __init__(self, s):\n",
    "        super().__init__()\n",
    "        self.Qa = ParamNormal((), scale=s)\n",
    "        sigma_plus = math.sqrt(s**2 + s**2)\n",
    "        self.Qb = ParamNormal((), scale=sigma_plus)\n",
    "\n",
    "        \n",
    "    def sample(self, N, shape=t.Size([])):\n",
    "        a = self.Qa().rsample(sample_shape=shape)\n",
    "        b = self.Qb().rsample(sample_shape=t.Size([*shape, N]))\n",
    "        \n",
    "        return (a.unsqueeze(-1), b)\n",
    "\n",
    "    \n",
    "    def log_prob(self, samples):\n",
    "        a, b = samples\n",
    "        logQa = self.Qa().log_prob(a)\n",
    "        logQb = self.Qb().log_prob(b)\n",
    "        \n",
    "        return logQa.sum(-1) + logQb.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logmeanexp(x, dim=0):\n",
    "    max_ = x.max(dim=dim, keepdim=True)[0]\n",
    "    normed = x - max_\n",
    "    lme = normed.exp() \\\n",
    "            .mean(dim, keepdim=True) \\\n",
    "            .log() \n",
    "\n",
    "    return (lme + max_).squeeze(dim)\n",
    "\n",
    "\n",
    "class SimpleTMC(nn.Module) :\n",
    "    def __init__(self, p, q, k):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        # TODO: allow variable k \n",
    "        self.K = k\n",
    "\n",
    "    \n",
    "    def get_error_on_a(self) :\n",
    "        return float(self.p.Pa.loc - self.q.Qa.loc)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        Qa, Qb = self.q.Qa, self.q.Qb\n",
    "        Pa, Pb, Px = self.p.Pa, self.p.Pb, self.p.Px\n",
    "        \n",
    "        a = Qa().rsample(sample_shape=t.Size([self.K, 1, 1]))\n",
    "        b = Qb().rsample(sample_shape=t.Size([self.K, x.size(0)]))\n",
    "        \n",
    "        fa = Pa().log_prob(a) - Qa().log_prob(a)\n",
    "        fb = Pb(a).log_prob(b) - Qb().log_prob(b)\n",
    "        fx = Px(b).log_prob(x)\n",
    "        \n",
    "        f_int_b = logmeanexp(fb + fx, dim=-2).sum(dim=-1) \\\n",
    "                    + fa.view(-1) # plate involved here\n",
    "        \n",
    "        return logmeanexp(f_int_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.501129150390625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# >10,000 epochs for serious eval\n",
    "def setup_and_run(tmc, x, ep=2000, eta=0.2) :\n",
    "    optimiser = t.optim.Adam(tmc.q.parameters(), lr=eta) # optimising q only\n",
    "    X = nn.Parameter(t.Tensor(x), requires_grad=False) \n",
    "\n",
    "    optimise(tmc, X, optimiser, ep)\n",
    "    \n",
    "    return tmc\n",
    "\n",
    "\n",
    "def optimise(tmc, x, optimiser, eps) :\n",
    "    for i in range(eps):\n",
    "        loss = - tmc(x)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimiser.step()\n",
    "        #print(tmc.q.Qa.loc.grad)\n",
    "\n",
    "\n",
    "def main(a_mu) :\n",
    "    sa = 1.\n",
    "    sb = sa\n",
    "    sx = sa\n",
    "    \n",
    "    N = 100\n",
    "    k = 5\n",
    "    \n",
    "    p = ChainP(sa, mean=a_mu)\n",
    "    q = ChainQ(sb)\n",
    "    tmc = SimpleTMC(p, q, k)\n",
    "    x, _, _ = tmc.p.sample(N)\n",
    "\n",
    "    tmc = setup_and_run(tmc, x)\n",
    "    \n",
    "    return tmc.get_error_on_a()\n",
    "\n",
    "a_mu = 100\n",
    "main(a_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2#30\n",
    "errors = np.array([main(a_mu) for i in range(N)])\n",
    "avg_error = errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average error:\", round(avg_error / a_mu * 100, 2), \"%\")\n",
    "print(\"Error variance:\", round(np.array(errors).var() / a_mu * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other attempts (more manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sample from Q, an approx posterior\n",
    "# Has to be an isotropic Gaussian, \n",
    "# also a nn.module\n",
    "def sample_model(k, c, prior_mean=0, var=1.0):\n",
    "    s = t.Size([k])\n",
    "    \n",
    "    a = Normal(prior_mean, var)\n",
    "    z_a = a.rsample(s)\n",
    "\n",
    "    b = Normal(z_a, var)\n",
    "    z_b = b.rsample()\n",
    "    \n",
    "    return z_a, z_b\n",
    "\n",
    "\n",
    "# TODO\n",
    "def get_factors(P, x) :\n",
    "    return P.log_prob(x)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "ks = [BATCH_SIZE] * N_VARS\n",
    "\n",
    "\n",
    "data = t.randn(1)\n",
    "samples = sample_model(BATCH_SIZE, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    estimators\n",
    "\"\"\"\n",
    "def vae_marginal_likelihood(x,z) :\n",
    "    return P(x,z) / Q(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
