{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMC by hand, in raw Pytorch \n",
    "\n",
    "e.g. Conjugate Gaussian chain\n",
    "\n",
    "no `trace` or combine\n",
    "\n",
    "Should be possible to do a chain model (\"just a list of matrices\")\n",
    " \n",
    " `a = N(0, s)`\n",
    " \n",
    " `b = N(a, s)`\n",
    " \n",
    " `c = N(b, s)`\n",
    " \n",
    " i.e. one known variable $c$: the data, at the end of the chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"tmc.png\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.distributions import MultivariateNormal as MVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIM = t.Size([1])\n",
    "N_VARS = 3\n",
    "\n",
    "# TODO: allow this to vary across variables\n",
    "BATCH_SIZE = 2 # samples per latent\n",
    "N_REC_LAYERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent a normal as two adaptive params and an eval method\n",
    "# from https://github.com/anonymous-78913/tmc-anon/blob/master/param/lvm.py\n",
    "class ParamNormal(nn.Module):\n",
    "    def __init__(self, sample_shape, scale=1.):\n",
    "        super().__init__()\n",
    "        self.loc = nn.Parameter(t.zeros(sample_shape))\n",
    "        self.log_scale = nn.Parameter(math.log(scale)*t.ones(sample_shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return Normal(self.loc, self.log_scale.exp())\n",
    "\n",
    "\n",
    "class LinearNormal(nn.Module):\n",
    "    def __init__(self, sample_shape=t.Size([]), scale=1.):\n",
    "        super().__init__()\n",
    "        self.log_scale = nn.Parameter(math.log(scale)*t.ones(sample_shape))\n",
    "\n",
    "    def forward(self, input_):\n",
    "        return Normal(input_, self.log_scale.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardcode a basic chain a -> b -> x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generative_model\n",
    "# TODO: let s vary\n",
    "class ChainP(nn.Module):\n",
    "    def __init__(self, s):\n",
    "        super().__init__()\n",
    "        self.Pa = ParamNormal((), scale=s)\n",
    "        self.Pb = LinearNormal((), scale=s)\n",
    "        self.Px = LinearNormal((), scale=s)\n",
    "\n",
    "    def sample(self, N):\n",
    "        a = self.Pa().sample()\n",
    "        b = self.Pb(a).sample(sample_shape=t.Size([N]))\n",
    "        x = self.Px(b).sample()\n",
    "        \n",
    "        return (x, (a.unsqueeze(-1), b))\n",
    "\n",
    "    def log_prob(self, samples):\n",
    "        x, (a, b) = samples\n",
    "        logPa   = self.Pa().log_prob(a)\n",
    "        logPb = self.Pb(a).log_prob(b)\n",
    "        logPx = self.Px(b).log_prob(x)\n",
    "        \n",
    "        return logPa.sum(-1) \\\n",
    "                + logPb.sum(-1) \\\n",
    "                + logPx.sum(-1)\n",
    "\n",
    "\n",
    "# TODO: isotropic?\n",
    "class ChainQ(nn.Module):\n",
    "    def __init__(self, s):\n",
    "        super().__init__()\n",
    "        self.Qa = ParamNormal((), scale=s)\n",
    "        sigma_plus = math.sqrt(s**2 + s**2)\n",
    "        self.Qb = ParamNormal((), scale=sigma_plus)\n",
    "\n",
    "    def sample(self, N, sample_shape=t.Size([])):\n",
    "        a = self.Qa().sample(sample_shape=sample_shape)\n",
    "        b = self.Qb().sample(sample_shape=t.Size([*sample_shape, N]))\n",
    "        \n",
    "        return (a.unsqueeze(-1), b)\n",
    "\n",
    "    def log_prob(self, samples):\n",
    "        a, b = samples\n",
    "        logQa = self.Qa().log_prob(a)\n",
    "        logQb = self.Qb().log_prob(b)\n",
    "        \n",
    "        return logQa.sum(-1) + logQb.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logmeanexp(x, dim=0):\n",
    "    max_ = x.max(dim=dim, keepdim=True)[0]\n",
    "    normed = x - max_\n",
    "    lme = normed.exp() \\\n",
    "            .mean(dim, keepdim=True) \\\n",
    "            .log() \n",
    "\n",
    "    return (lme + max_).squeeze(dim)\n",
    "\n",
    "\n",
    "class SimpleTMC(nn.Module) :\n",
    "    def __init__(self, p, q, k):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        # TODO: allow variable k \n",
    "        self.K = k\n",
    "    \n",
    "    \"\"\"\n",
    "    def reduce_factors(self, log_ps) :\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def tmc_marginal_likelihood(self, log_probs, ks) :\n",
    "        norm = 1 / np.prod(ks)\n",
    "\n",
    "        return norm * reduce_factors(log_probs)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        a  = self.q.Qa().sample(sample_shape=t.Size([self.K, 1, 1]))\n",
    "        b  = self.q.Qb().sample(sample_shape=t.Size([self.K, x.size(0)]))\n",
    "        \n",
    "        fa = self.p.Pa().log_prob(a) - self.q.Qa().log_prob(a)\n",
    "        fb = self.p.Pb(a).log_prob(b) - self.q.Qb().log_prob(b)\n",
    "        fx = self.p.Px(b).log_prob(x)\n",
    "        \n",
    "        f_int_b = logmeanexp(fb + fx, -2)\n",
    "        f_int_b = f_int_b.sum(-1) + fa.view(-1)\n",
    "        f_int_a = logmeanexp(f_int_b)\n",
    "\n",
    "        return f_int_a\n",
    "    \n",
    "    \n",
    "    def loss(self, **kwargs) :\n",
    "        # log_p_x_z = \n",
    "        # kld_loss = \n",
    "        # importance weights\n",
    "        # Rescale the weights (along the sample dim) to lie in [0, 1] and sum to 1\n",
    "        # scaled = torch.sum(weight * log_weight, dim=-1)\n",
    "        #loss_ = torch.mean()\n",
    "\n",
    "        return #loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-219.23984"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "k = 2\n",
    "n_vars = 3\n",
    "iters = 100\n",
    "\n",
    "# TODO: separate\n",
    "sa = 1.\n",
    "sb = sa\n",
    "sx = sa\n",
    "\n",
    "\n",
    "p = ChainP(sa)\n",
    "q = ChainQ(sb)\n",
    "tmc = SimpleTMC(p, q, k)\n",
    "x, _ = tmc.p.sample(N)\n",
    "\n",
    "tmcs = []\n",
    "for i in range(iters):\n",
    "    t.manual_seed(i)\n",
    "    res = tmc(x)\n",
    "    tmcs.append(res.detach().numpy())\n",
    "    \n",
    "\n",
    "np.array(tmcs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-45d7c195e5a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtmc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-45d7c195e5a0>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(x, ep, eta)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleTMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3000\n",
    "\n",
    "def run(x, ep=5000, eta=0.2) :\n",
    "    tmc = SimpleTMC(p, q, k)\n",
    "    optimiser = t.optim.Adam(tmc.parameters(), lr=eta)\n",
    "    X = Variable(t.Tensor(x), requires_grad=False) \n",
    "\n",
    "    optimise(tmc, x, optimiser, ep)\n",
    "    \n",
    "    return tmc\n",
    "\n",
    "\n",
    "def optimise(tmc, x, optimiser, eps, verbose=False) :\n",
    "    for i in range(eps):\n",
    "        loss = - tmc.loss(x)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimiser.step()\n",
    "\n",
    "        if verbose :\n",
    "            if i % 500 == 0:\n",
    "                print(q.get_mean(), q.get_var())\n",
    "\n",
    "\n",
    "tmc = run(x, ep=EPOCHS)\n",
    "tmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other attempts (more manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sample from Q, an approx posterior\n",
    "# Has to be an isotropic Gaussian, \n",
    "# also a nn.module\n",
    "def sample_model(k, c, prior_mean=0, var=1.0):\n",
    "    s = t.Size([k])\n",
    "    \n",
    "    a = Normal(prior_mean, var)\n",
    "    z_a = a.rsample(s)\n",
    "\n",
    "    b = Normal(z_a, var)\n",
    "    z_b = b.rsample()\n",
    "    \n",
    "    # TODO: ?\n",
    "    c \n",
    "    \n",
    "    return z_a, z_b\n",
    "\n",
    "\n",
    "# TODO\n",
    "def get_factors(P, x) :\n",
    "    return P.log_prob(x)\n",
    "\n",
    "\n",
    "ks = [BATCH_SIZE] * N_VARS\n",
    "\n",
    "\n",
    "data = t.randn(1)\n",
    "samples = sample_model(BATCH_SIZE, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-146.6355)\n"
     ]
    }
   ],
   "source": [
    "class BaseClassAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseClassAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: t.Tensor) -> list:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: t.Tensor) :\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> t.Tensor:\n",
    "        raise RuntimeWarning()\n",
    "\n",
    "    def generate(self, x: t.Tensor, **kwargs) -> t.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, *inputs: t.Tensor) -> t.Tensor:\n",
    "        pass\n",
    "\n",
    "    def loss_function(self, *inputs, **kwargs) -> t.Tensor:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    estimators\n",
    "\"\"\"\n",
    "def vae_marginal_likelihood(x,z) :\n",
    "    return P(x,z) / Q(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
