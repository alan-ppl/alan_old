{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI without plates\n",
    "\n",
    "i.e. no repeating bits to abstract over\n",
    "\n",
    "Optimise the params of an approx posterior over extended Z-space, but not K space\n",
    "\n",
    "$$Q (Z|X) = \\prod_k  Q(Z^k|X) = \\prod_k \\prod_i Q(Z^k_i \\mid Z^k_{qa(i)})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\prod_j f_j^{\\kappa_j} = \\frac{P(x_, Z)}{\\prod Q(z_i^{k_i})}$$\n",
    "\n",
    "Writing out the target (log marginal likelihood) fully makes the computation clear:\n",
    "\n",
    "$$ \\mathcal{L}= E_{Q(Z|X)} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "$$= E_{Q} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "  \\frac{P({Z, K, X})}{Q({Z|X})} = \n",
    "  P({K}) \n",
    "  P \\left({X| Z_{\\mathrm{pa}{X}}^{K_{\\mathrm{pa}{X}}}} \\right)  \n",
    "  \\prod_i \n",
    "  \\frac{P\\left({Z_i^{K_i}| Z^{K_{\\mathrm{pa}(i)}}_{\\mathrm{pa}(i)}} \\right)}\n",
    "  {Q \\left(  \n",
    "    Z_i^{K_i}| Z^{K_i}_{\\mathrm{qa}(i)}\n",
    "  \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The computation\n",
    "\n",
    "1. Form joint P/Q: index prior * lik * product of latent P/Qs\n",
    "2. $\\mathcal{L}$: sum out K, then log P - log Q, then average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.distributions import MultivariateNormal as MVN\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from tpp_trace import *\n",
    "import utils as u\n",
    "import tensor_ops as tpp\n",
    "from tvi import *\n",
    "\n",
    "import imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## First with no plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a factorised approx posterior. generate 3 simple variables\n",
    "# sample along the chain\n",
    "\n",
    "# a ~ N([1],[3])\n",
    "# b ~ N(a,[3])\n",
    "# c ~ N(b,[3])\n",
    "\n",
    "n = 3\n",
    "scale = n\n",
    "Norm = lambda mu, var : WrappedDist(Normal, mu, var)\n",
    "\n",
    "TRUE_MEAN_A = 10\n",
    "\n",
    "# Prior\n",
    "# a -> b -> c observed\n",
    "def chain_dist(trace, n=3):\n",
    "    a = trace[\"a\"](Norm(t.ones(n) * TRUE_MEAN_A, scale))\n",
    "    b = trace[\"b\"](Norm(a, scale))\n",
    "    c = trace[\"c\"](Norm(b, scale))\n",
    "    \n",
    "    return c\n",
    "\n",
    "# a param placeholder\n",
    "# Hardcoding 2 params for each var, for now\n",
    "# factorised Gaussian with learned means and covs\n",
    "class ChainQ(nn.Module):\n",
    "    def __init__(self, n=3):\n",
    "        super().__init__()\n",
    "        self.mean_a = nn.Parameter(t.ones(n))\n",
    "        self.mean_b = nn.Parameter(t.ones(n))\n",
    "        self.logscale_a = nn.Parameter(t.ones(n)) # t.log(t.ones(n))\n",
    "        self.logscale_b = nn.Parameter(t.ones(n))\n",
    "    \n",
    "    # TODO: make this actually depend on the params\n",
    "    def sample(self, trace) :\n",
    "        a = trace[\"a\"](WrappedDist(Normal, self.mean_a, t.exp(self.logscale_a)))#Norm(self.mean_a, t.exp(self.logscale_a)))\n",
    "        b = trace[\"b\"](WrappedDist(Normal, self.mean_b, t.exp(self.logscale_b)))#Norm(a * self.mean_b, t.exp(self.logscale_b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TVI(nn.Module) :\n",
    "    def __init__(self, p, q, k, x, nProtectedDims):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.nProtected = nProtectedDims\n",
    "        \n",
    "        self.data_dict = {}\n",
    "        self.data_dict[\"__c\"] = []\n",
    "        self.data = nn.Parameter(x, requires_grad=False) \n",
    "        \n",
    "    # 1. s = sample Q\n",
    "    # 2. lp_Q = eval Q.logprob(s)\n",
    "    # 3. lp_P = eval P.logprob(s)\n",
    "    # 4. f = lp_P - lp_Q\n",
    "    # 5. loss = combine fs\n",
    "    def forward(self):\n",
    "        self.data_dict[\"__c\"].append(self.data)\n",
    "        \n",
    "        # init traces at each step\n",
    "        sample_trace = sampler(self.k, self.nProtected, data={\"__c\": self.data})\n",
    "        # sample recognition model Q -> Q-sample and Q-logprobs\n",
    "        self.q.sample(sample_trace)\n",
    "        # compute P logprobs under prior\n",
    "        #self.p(sample_trace)\n",
    "        \n",
    "        # Pass Q samples to new trace\n",
    "        eval_trace = evaluator(sample_trace, self.nProtected, data={\"__c\": self.data})\n",
    "        # compute P logprobs under ...\n",
    "        self.p(eval_trace)\n",
    "        \n",
    "        eval_trace.sum_out_pos()\n",
    "        sample_trace.sum_out_pos()\n",
    "        # align dims in Q\n",
    "        sample_trace.trace.out_dicts = rename_placeholders(eval_trace, sample_trace)\n",
    "        \n",
    "        # to ratio land: P.log_probs - Q.log_probs (just the latents)\n",
    "        tensors = subtract_latent_log_probs(eval_trace, sample_trace)\n",
    "        \n",
    "        # REMOVE after debug\n",
    "        # Try using just P for loss\n",
    "        #tensors = eval_trace.trace.out_dicts[\"log_prob\"]\n",
    "        \n",
    "        # reduce gives loss\n",
    "        loss_dict = tpp.combine_tensors(tensors)\n",
    "        assert(len(loss_dict.keys()) == 1)\n",
    "        key = next(iter(loss_dict))\n",
    "\n",
    "        return loss_dict[key]\n",
    "\n",
    "\n",
    "def setup_and_run(tvi, ep=2000, eta=1) :\n",
    "    print(\"Q params init:\", list(iter(tvi.q.parameters())))\n",
    "    optimiser = t.optim.Adam(tvi.q.parameters(), lr=eta) # optimising q only    \n",
    "    optimise(tvi, optimiser, ep)\n",
    "    \n",
    "    return tvi\n",
    "\n",
    "\n",
    "def optimise(tvi, optimiser, eps) :\n",
    "    for i in range(eps):\n",
    "        optimiser.zero_grad()\n",
    "        loss = - tvi() \n",
    "        loss.backward()# retain_graph=True\n",
    "\n",
    "        #for name, param in tvi.named_parameters():\n",
    "        #    print(name, param.grad)\n",
    "\n",
    "        optimiser.step()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def sample_generator(nProtected, P, dataName=\"__c\") :\n",
    "    k = 1\n",
    "    trp = sampler(k, nProtected, data={})\n",
    "    P(trp)\n",
    "    return trp.trace.out_dicts[\"sample\"][dataName] \\\n",
    "            .squeeze(0)\n",
    "        \n",
    "\n",
    "def get_error_on_a(a_mean, n, tvi) :\n",
    "    a_mean = t.ones(n) * a_mean\n",
    "    return a_mean - tvi.q.mean_a\n",
    "\n",
    "\n",
    "# Recovering mean of first var\n",
    "def main(nvars=3, nProtected=2, k=2, epochs=2000, true_mean=10, lr=0.2) :\n",
    "    Q = ChainQ()\n",
    "    P = chain_dist\n",
    "    \n",
    "    # Get _c data by sampling generator\n",
    "    x = sample_generator(nProtected, P, dataName=\"__c\")\n",
    "    tvi = setup_and_run(TVI(P, Q, k, x, nProtected), epochs, eta=lr)\n",
    "    \n",
    "    return get_error_on_a(true_mean, nvars, tvi), tvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q params init: [Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)]\n",
      "\n",
      "tensor([-1.3009, -4.8312, -1.0335], grad_fn=<MulBackward0>)%\n"
     ]
    }
   ],
   "source": [
    "ep = 5000 \n",
    "error, tvi = main(nvars=3, k=2, epochs=ep, true_mean=TRUE_MEAN_A, lr=0.1)\n",
    "\n",
    "#tvi.q.mean_a, tvi.q.mean_b, tvi.q.logscale_a, tvi.q.logscale_b#, tvi.data_dict\n",
    "print()\n",
    "print(f\"{error / TRUE_MEAN_A *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Q = ChainQ()\n",
    "P = chain_dist\n",
    "k = 2\n",
    "nProtected = 2 \n",
    "\n",
    "# Get _c data by sampling generator\n",
    "x = sample_generator(nProtected, P, dataName=\"__c\")\n",
    "\n",
    "trq = sampler(k, nProtected, data={\"__c\": x})\n",
    "#print(trq.trace.in_dicts)\n",
    "Q.sample(trq)\n",
    "#print(trq.trace.out_dicts)\n",
    "trp = evaluator(trq, nProtected, data={\"__c\": x})\n",
    "#P(trp) # evaluate under prior!\n",
    "#print(trp.trace.in_dicts)\n",
    "# compute P logprobs\n",
    "P(trp)\n",
    "#print(trp.trace.out_dicts)\n",
    "#print(trq.trace.fn.plate_names)\n",
    "\n",
    "trp.sum_out_pos()\n",
    "trq.sum_out_pos()\n",
    "# aligning dims in Q\n",
    "trq.trace.out_dicts = rename_placeholders(trp, trq)\n",
    "\n",
    "# to ratio land: P.log_probs - Q.log_probs (just the latents)\n",
    "tensors = subtract_latent_log_probs(trp, trq)\n",
    "#print(\"\\n\", tensors)\n",
    "# reduce gives loss\n",
    "loss_dict = tpp.combine_tensors(tensors)\n",
    "assert(len(loss_dict.keys()) == 1)\n",
    "key = next(iter(loss_dict))\n",
    "\n",
    "loss_dict[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Temp, rain, hum\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# apples and oranges\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# weights and Biases\n",
    "w = torch.randn(2,3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)\n",
    "\n",
    "\n",
    "#  Define model\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "predit = model(inputs)\n",
    "\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff)/ diff.numel()\n",
    "\n",
    "loss = mse(predit, targets)\n",
    "loss\n",
    "\n",
    "\n",
    "# Compute gradient\n",
    "w.retain_grad()\n",
    "b.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "print(w)\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI, No plates but including deletes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chain_dist_del(trace):\n",
    "    a = trace[\"a\"](Norm(t.ones(n)))\n",
    "    b = trace[\"b\"](Norm(a))\n",
    "    c = trace[\"c\"](Norm(b))\n",
    "    (c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "    d = trace[\"d\"](Norm(c))\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# call sampler on Q. \n",
    "# gives you the samples and a log Q tensor `log_prob`\n",
    "tr1 = sampler(draws, nProtected, data=data)\n",
    "\n",
    "val = P(tr1)\n",
    "log_q = tr1.trace.out_dicts[\"log_prob\"]\n",
    "\n",
    "# compute the log_probs\n",
    "\n",
    "# pass these to evaluator, which does a lookup for all the latents \n",
    "# gives you log P for each latent\n",
    "tr2 = evaluator(tr1, nProtected, data=data)\n",
    "val = P(tr2)\n",
    "\n",
    "#tr2.trace.out_dicts[\"log_prob\"]\n",
    "#log_p = \n",
    "\n",
    "#Q = pytorch.module\n",
    "#    - `q.forward()` will look like chain_dist\n",
    "    \n",
    "\n",
    "#- optimise it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plate VI\n",
    "\n",
    "- For plates, we just don't filter [@17](https://github.com/LaurenceA/tpp/blob/bd1fe20dcf86a1c02cc0424632571fba998d104f/utils.py#L17)\n",
    "- Painful stuff: need to keep the generative order (e.g. a, b, c, d)\n",
    "    - because we start by summing the lowest-level plates\n",
    "        - solution: enforce that the last variable is a leaf e.g. `d`\n",
    "- Careful when combining P & Q tensors: maintain the ordering!\n",
    "\n",
    "- Plates: doing the summation backwards through the plates, yeah?\n",
    "    - This implies tricky implementation blah\n",
    "    - Py 3.6 dicts are ordered by insertion though, so use that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example directed graph with plate repeats\n",
    "# 3(a) -> 4(b) -> c -> d\n",
    "def plate_dist(trace):\n",
    "    Na = Norm(t.ones(n))\n",
    "    a = trace[\"a\"](Na, plate_name=\"A\", plate_shape=3)\n",
    "    Nb = Norm(a)\n",
    "    b = trace[\"b\"](Nb, plate_name=\"B\", plate_shape=4)\n",
    "    Nc = Norm(b)\n",
    "    c = trace[\"c\"](Nc)\n",
    "    \n",
    "    (c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "    \n",
    "    Nd = Norm(c)\n",
    "    d = trace[\"d\"](Nd)\n",
    "    \n",
    "    return d\n",
    "\n",
    "tr = sample_and_eval(plate_dist, draws=1, nProtected=2)\n",
    "tr.trace.out_dicts\n",
    "tr.trace.in_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
