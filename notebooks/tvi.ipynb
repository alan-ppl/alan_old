{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI without plates\n",
    "\n",
    "i.e. no repeating bits to abstract over\n",
    "\n",
    "Optimise the params of an approx posterior over extended Z-space, but not K space\n",
    "\n",
    "$$Q (Z|X) = \\prod_k  Q(Z^k|X) = \\prod_k \\prod_i Q(Z^k_i \\mid Z^k_{qa(i)})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\prod_j f_j^{\\kappa_j} = \\frac{P(x_, Z)}{\\prod Q(z_i^{k_i})}$$\n",
    "\n",
    "Writing out the target (log marginal likelihood) fully makes the computation clear:\n",
    "\n",
    "$$ \\mathcal{L}= E_{Q(Z|X)} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "$$= E_{Q} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "  \\frac{P({Z, K, X})}{Q({Z|X})} = \n",
    "  P({K}) \n",
    "  P \\left({X| Z_{\\mathrm{pa}{X}}^{K_{\\mathrm{pa}{X}}}} \\right)  \n",
    "  \\prod_i \n",
    "  \\frac{P\\left({Z_i^{K_i}| Z^{K_{\\mathrm{pa}(i)}}_{\\mathrm{pa}(i)}} \\right)}\n",
    "  {Q \\left(  \n",
    "    Z_i^{K_i}| Z^{K_i}_{\\mathrm{qa}(i)}\n",
    "  \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The computation\n",
    "\n",
    "1. Form joint P/Q: index prior * lik * product of latent P/Qs\n",
    "2. $\\mathcal{L}$: sum out K, then log P - log Q, then average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.distributions import MultivariateNormal as MVN\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from tpp_trace import *\n",
    "import utils as u\n",
    "import tensor_ops as tpp\n",
    "from tvi import *\n",
    "\n",
    "import imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First with no plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a factorised approx posterior. generate 3 simple variables\n",
    "# sample along the chain\n",
    "\n",
    "# a ~ N([1],[3])\n",
    "# b ~ N(a,[3])\n",
    "# c ~ N(b,[3])\n",
    "\n",
    "n = 3\n",
    "scale = n\n",
    "Norm = lambda mu : WrappedDist(Normal, mu, scale)\n",
    "\n",
    "TRUE_MEAN_A = 10\n",
    "\n",
    "# a -> b -> c observed\n",
    "def chain_dist(trace, n=3):\n",
    "    a = trace[\"a\"](Norm(t.ones(n) * TRUE_MEAN_A))\n",
    "    b = trace[\"b\"](Norm(a))\n",
    "    c = trace[\"c\"](Norm(b))\n",
    "    \n",
    "    return c\n",
    "\n",
    "# a param placeholder\n",
    "# Hardcoding 2 params for each var, for now\n",
    "# factorised Gaussian with learned means and covs\n",
    "class ChainQ(nn.Module):\n",
    "    def __init__(self, n=3):\n",
    "        super().__init__()\n",
    "        self.mean_a = nn.Parameter(t.zeros(n))\n",
    "        self.mean_b = nn.Parameter(t.zeros(n))\n",
    "        self.logscale_a = nn.Parameter(t.zeros(n))\n",
    "        self.logscale_b = nn.Parameter(t.zeros(n))\n",
    "    \n",
    "    def sample(self, trace) :\n",
    "        a = trace[\"a\"](Norm(t.ones(n)))\n",
    "        b = trace[\"b\"](Norm(a)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dryrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draws = 2\n",
    "nProtected = 2 # (number of vars in chain dist) minus observed nodes\n",
    "\n",
    "P = chain_dist\n",
    "# TODO: Why pos_B?\n",
    "data_tensor = (t.ones(n) * 1).refine_names('pos_B')\n",
    "data = {\"__c\": data_tensor}\n",
    "\n",
    "trq = sampler(draws, nProtected, data=data)\n",
    "Q = ChainQ()\n",
    "Q.sample(trq)\n",
    "trp = evaluator(trq, nProtected, data=data)\n",
    "P(trp)\n",
    "print(trp.trace.out_dicts)\n",
    "P(trq)\n",
    "trq.sum_out_pos()\n",
    "trp.sum_out_pos()\n",
    "\n",
    "trq.trace.out_dicts = rename_placeholders(trp, trq)\n",
    "\n",
    "#print(trp.trace.out_dicts[\"sample\"])\n",
    "#print()\n",
    "#print(trq.trace.out_dicts[\"sample\"])\n",
    "#print()\n",
    "tensors = subtract_latent_log_probs(trp, trq)\n",
    "loss_dict = tpp.combine_tensors(tensors)\n",
    "key = next(iter(loss_dict))\n",
    "loss_dict[key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TVI(nn.Module) :\n",
    "    def __init__(self, p, q, k, x, nProtectedDims):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.nProtected = nProtectedDims\n",
    "        \n",
    "        self.data_dict = {}\n",
    "        self.data_dict[\"__c\"] = []\n",
    "        self.data = nn.Parameter(x, requires_grad=False) \n",
    "        \n",
    "    # 1. s = sample Q\n",
    "    # 2. lp_Q = eval Q.logprob(s)\n",
    "    # 3. lp_P = eval P.logprob(s)\n",
    "    # 4. f = lp_P - lp_Q\n",
    "    # 5. loss = combine fs\n",
    "    def forward(self):\n",
    "        self.data_dict[\"__c\"].append(self.data)\n",
    "        \n",
    "        # init traces at each step\n",
    "        sample_trace = sampler(self.k, self.nProtected, data={\"__c\": self.data})\n",
    "        # sample recognition model Q -> Q-sample and Q-logprobs\n",
    "        self.q.sample(sample_trace)\n",
    "        # compute P logprobs under prior\n",
    "        #self.p(sample_trace)\n",
    "        \n",
    "        # Pass Q samples to new trace\n",
    "        eval_trace = evaluator(sample_trace, self.nProtected, data={\"__c\": self.data})\n",
    "        # compute P logprobs under ...\n",
    "        self.p(eval_trace)\n",
    "        \n",
    "        eval_trace.sum_out_pos()\n",
    "        sample_trace.sum_out_pos()\n",
    "        # align dims in Q\n",
    "        sample_trace.trace.out_dicts = rename_placeholders(eval_trace, sample_trace)\n",
    "        \n",
    "        # to ratio land: P.log_probs - Q.log_probs (just the latents)\n",
    "        tensors = subtract_latent_log_probs(eval_trace, sample_trace)\n",
    "        # reduce gives loss\n",
    "        loss_dict = tpp.combine_tensors(tensors)\n",
    "        assert(len(loss_dict.keys()) == 1)\n",
    "        key = next(iter(loss_dict))\n",
    "\n",
    "        return loss_dict[key]\n",
    "\n",
    "\n",
    "def setup_and_run(tvi, ep=2000, eta=1) :\n",
    "    print(\"Q params init:\", [ p for p in tvi.q.parameters() ])\n",
    "    optimiser = t.optim.Adam(tvi.q.parameters(), lr=eta) # optimising q only    \n",
    "    optimise(tvi, optimiser, ep)\n",
    "    \n",
    "    return tvi\n",
    "\n",
    "\n",
    "def optimise(tvi, optimiser, eps) :\n",
    "    for i in range(eps):\n",
    "        optimiser.zero_grad()\n",
    "        loss = - tvi() \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimiser.step()\n",
    "\n",
    "\n",
    "def sample_generator(nProtected, P, dataName=\"__c\") :\n",
    "    k = 1\n",
    "    trp = sampler(k, nProtected, data={})\n",
    "    P(trp)\n",
    "    return trp.trace.out_dicts[\"sample\"][dataName] \\\n",
    "            .squeeze(0)\n",
    "        \n",
    "\n",
    "def get_error_on_a(a_mean, n, tvi) :\n",
    "    a_mean = t.ones(n) * a_mean\n",
    "    return a_mean - tvi.q.mean_a\n",
    "\n",
    "\n",
    "# Recovering mean of first var\n",
    "def main(nvars=3, nProtected=2, k=2, epochs=2000, true_mean=10, lr=0.2) :\n",
    "    Q = ChainQ()\n",
    "    P = chain_dist\n",
    "    \n",
    "    # Get _c data by sampling generator\n",
    "    x = sample_generator(nProtected, P, dataName=\"__c\")\n",
    "    tvi = setup_and_run(TVI(P, Q, k, x, nProtected), epochs, eta=lr)\n",
    "    \n",
    "    return get_error_on_a(true_mean, nvars, tvi), tvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q params init: [Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)]\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "error, tvi = main(nvars=3, k=2, epochs=1000, true_mean=TRUE_MEAN_A, lr=0.2)\n",
    "print(tvi.q.mean_a)\n",
    "#, tvi.q.mean_b, tvi.q.logscale_a, tvi.q.logscale_b#, tvi.data_dict\n",
    "\n",
    "#print(f\"{error / TRUE_MEAN_A *100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample': {'__a': tensor([[[ 0.9192,  3.0368, -3.8186]],\n",
      "\n",
      "        [[ 3.4921, -2.6384,  4.6960]]], names=('_K', 'pos_A', 'pos_B')), '__b': tensor([[[-0.2439,  1.9266, -4.3716]],\n",
      "\n",
      "        [[ 4.5765, -3.4173, -2.1387]]], names=('_K', 'pos_A', 'pos_B'))}, 'log_prob': {'__a': tensor([[[-2.0179, -2.2480, -3.3075]],\n",
      "\n",
      "        [[-2.3626, -2.7530, -2.7764]]], names=('_K', 'pos_A', 'pos_B')), '__b': tensor([[[-2.0927, -2.0860, -2.0345]],\n",
      "\n",
      "        [[-2.0829, -2.0513, -4.6127]]], names=('_K', 'pos_A', 'pos_B'))}}\n",
      "{'data': {'__c': tensor([[14.0023, 14.0198, 11.2501]], names=('pos_A', 'pos_B'))}, 'sample': {'__a': tensor([[[ 0.9192,  3.0368, -3.8186]],\n",
      "\n",
      "        [[ 3.4921, -2.6384,  4.6960]]], names=('_K', 'pos_A', 'pos_B')), '__b': tensor([[[-0.2439,  1.9266, -4.3716]],\n",
      "\n",
      "        [[ 4.5765, -3.4173, -2.1387]]], names=('_K', 'pos_A', 'pos_B'))}}\n",
      "{'sample': {'__a': tensor([[[ 0.9192,  3.0368, -3.8186]],\n",
      "\n",
      "        [[ 3.4921, -2.6384,  4.6960]]], names=('_k__a', 'pos_A', 'pos_B')), '__b': tensor([[[[-0.2439,  1.9266, -4.3716]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5765, -3.4173, -2.1387]]]],\n",
      "       names=('_k__b', '_k__a', 'pos_A', 'pos_B'))}, 'log_prob': {'__a': tensor([[[ -6.5988,  -4.7112, -12.6261]],\n",
      "\n",
      "        [[ -4.3705, -10.8914,  -3.5805]]], names=('_k__a', 'pos_A', 'pos_B')), '__b': tensor([[[[-2.0927, -2.0860, -2.0345]],\n",
      "\n",
      "         [[-2.7930, -3.1753, -6.5853]]],\n",
      "\n",
      "\n",
      "        [[[-2.7607, -4.3317, -2.1743]],\n",
      "\n",
      "         [[-2.0829, -2.0513, -4.6127]]]],\n",
      "       names=('_k__b', '_k__a', 'pos_A', 'pos_B')), '__c': tensor([[[[-13.2928, -10.1422, -15.5751]]],\n",
      "\n",
      "\n",
      "        [[[ -6.9534, -18.9093, -11.9764]]]],\n",
      "       names=('_k__b', '_k__a', 'pos_A', 'pos_B'))}}\n",
      "\n",
      " {'__a': tensor([-16.3627, -10.9504], names=('_k__a',)), '__b': tensor([[ 0.0000, -6.3404],\n",
      "        [-0.5199,  0.0000]], names=('_k__b', '_k__a'))}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-12.3278, requires_grad=True)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = ChainQ()\n",
    "P = chain_dist\n",
    "k = 2\n",
    "nProtected = 2 \n",
    "\n",
    "# Get _c data by sampling generator\n",
    "x = sample_generator(nProtected, P, dataName=\"__c\")\n",
    "\n",
    "trq = sampler(k, nProtected, data={\"__c\": x})\n",
    "#print(trq.trace.in_dicts)\n",
    "Q.sample(trq)\n",
    "print(trq.trace.out_dicts)\n",
    "trp = evaluator(trq, nProtected, data={\"__c\": x})\n",
    "#P(trp) # evaluate under prior!\n",
    "print(trp.trace.in_dicts)\n",
    "# compute P logprobs\n",
    "P(trp)\n",
    "print(trp.trace.out_dicts)\n",
    "#print(trq.trace.fn.plate_names)\n",
    "\n",
    "trp.sum_out_pos()\n",
    "trq.sum_out_pos()\n",
    "# aligning dims in Q\n",
    "trq.trace.out_dicts = rename_placeholders(trp, trq)\n",
    "\n",
    "# to ratio land: P.log_probs - Q.log_probs (just the latents)\n",
    "tensors = subtract_latent_log_probs(trp, trq)\n",
    "print(\"\\n\", tensors)\n",
    "# reduce gives loss\n",
    "loss_dict = tpp.combine_tensors(tensors)\n",
    "assert(len(loss_dict.keys()) == 1)\n",
    "key = next(iter(loss_dict))\n",
    "\n",
    "loss_dict[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample': {'__a': tensor([[[11.0944, 10.5638,  9.4949]],\n",
       "  \n",
       "          [[ 5.8477,  8.7341,  9.2536]]], names=('_k__a', 'pos_A', 'pos_B')),\n",
       "  '__b': tensor([[[[13.9619, 16.5348,  3.4176]]],\n",
       "  \n",
       "  \n",
       "          [[[ 7.3123,  8.6786,  9.7241]]]],\n",
       "         names=('_k__b', '_k__a', 'pos_A', 'pos_B')),\n",
       "  '__c': tensor([[[[[ 8.9255, 16.4849,  2.0058]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ 7.1843,  4.3577, 11.1260]]]]],\n",
       "         names=('_k__c', '_k__b', '_k__a', 'pos_A', 'pos_B'))},\n",
       " 'log_prob': {'__a': tensor([[[-2.0841, -2.0352, -2.0317]],\n",
       "  \n",
       "          [[-2.9754, -2.1066, -2.0485]]], names=('_k__a', 'pos_A', 'pos_B')),\n",
       "  '__b': tensor([[[[-2.4744, -3.9983, -4.0694]],\n",
       "  \n",
       "           [[-5.6753, -5.3981, -3.9097]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.8122, -2.2150, -2.0205]],\n",
       "  \n",
       "           [[-2.1367, -2.0177, -2.0298]]]],\n",
       "         names=('_k__b', '_k__a', 'pos_A', 'pos_B')),\n",
       "  '__c': tensor([[[[[ -3.4267,  -2.0177,  -2.1283]]],\n",
       "  \n",
       "  \n",
       "           [[[ -2.1621,  -5.4030,  -5.3271]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "          [[[[ -4.5696, -10.2555,  -5.3186]]],\n",
       "  \n",
       "  \n",
       "           [[[ -2.0185,  -3.0548,  -2.1267]]]]],\n",
       "         names=('_k__c', '_k__b', '_k__a', 'pos_A', 'pos_B'))}}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr1 = trace({\"data\": {}}, SampleLogProbK(K=2, protected_dims=2))\n",
    "d = WrappedDist(Normal, t.ones(3), 3)\n",
    "a = tr1[\"a\"](d) \n",
    "val = chain_dist(tr1)\n",
    "tr2 = trace({\"data\": {}, \"sample\": tr1.trace.out_dicts[\"sample\"]}, LogProbK(tr1.trace.fn.plate_names, 2))\n",
    "val = chain_dist(tr2)\n",
    "\n",
    "tr2.trace.out_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI, No plates but including deletes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chain_dist_del(trace):\n",
    "    a = trace[\"a\"](Norm(t.ones(n)))\n",
    "    b = trace[\"b\"](Norm(a))\n",
    "    c = trace[\"c\"](Norm(b))\n",
    "    (c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "    d = trace[\"d\"](Norm(c))\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# call sampler on Q. \n",
    "# gives you the samples and a log Q tensor `log_prob`\n",
    "tr1 = sampler(draws, nProtected, data=data)\n",
    "\n",
    "val = P(tr1)\n",
    "log_q = tr1.trace.out_dicts[\"log_prob\"]\n",
    "\n",
    "# compute the log_probs\n",
    "\n",
    "# pass these to evaluator, which does a lookup for all the latents \n",
    "# gives you log P for each latent\n",
    "tr2 = evaluator(tr1, nProtected, data=data)\n",
    "val = P(tr2)\n",
    "\n",
    "#tr2.trace.out_dicts[\"log_prob\"]\n",
    "#log_p = \n",
    "\n",
    "#Q = pytorch.module\n",
    "#    - `q.forward()` will look like chain_dist\n",
    "    \n",
    "\n",
    "#- optimise it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plate VI\n",
    "\n",
    "- For plates, we just don't filter [@17](https://github.com/LaurenceA/tpp/blob/bd1fe20dcf86a1c02cc0424632571fba998d104f/utils.py#L17)\n",
    "- Painful stuff: need to keep the generative order (e.g. a, b, c, d)\n",
    "    - because we start by summing the lowest-level plates\n",
    "        - solution: enforce that the last variable is a leaf e.g. `d`\n",
    "- Careful when combining P & Q tensors: maintain the ordering!\n",
    "\n",
    "- Plates: doing the summation backwards through the plates, yeah?\n",
    "    - This implies tricky implementation blah\n",
    "    - Py 3.6 dicts are ordered by insertion though, so use that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example directed graph with plate repeats\n",
    "# 3(a) -> 4(b) -> c -> d\n",
    "def plate_dist(trace):\n",
    "    Na = Norm(t.ones(n))\n",
    "    a = trace[\"a\"](Na, plate_name=\"A\", plate_shape=3)\n",
    "    Nb = Norm(a)\n",
    "    b = trace[\"b\"](Nb, plate_name=\"B\", plate_shape=4)\n",
    "    Nc = Norm(b)\n",
    "    c = trace[\"c\"](Nc)\n",
    "    \n",
    "    (c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "    \n",
    "    Nd = Norm(c)\n",
    "    d = trace[\"d\"](Nd)\n",
    "    \n",
    "    return d\n",
    "\n",
    "tr = sample_and_eval(plate_dist, draws=1, nProtected=2)\n",
    "tr.trace.out_dicts\n",
    "tr.trace.in_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
