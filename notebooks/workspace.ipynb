{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three types of dimension:\n",
    "* plate dims\n",
    "* sample dimensions (usually indexed K)\n",
    "* user dims; underlying dimensions (which the user gets to interact with). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from tpp_trace import *\n",
    "import utils as u\n",
    "import tensor_ops as tpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plates: []\n",
      "('_K', 'pos_A', 'pos_B')\n",
      "('_k__d', '_k__c', 'pos_A', 'pos_B')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleech/anaconda3/lib/python3.6/site-packages/torch/tensor.py:723: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:840.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2551,  0.9730,  0.8046]],\n",
       "\n",
       "        [[-3.6178,  1.9766, -7.3755]],\n",
       "\n",
       "        [[ 1.4900, -6.8641,  0.2563]]], names=('_k__a', 'pos_A', 'pos_B'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa = 3\n",
    "n = 2\n",
    "tr = sample_and_eval(chain_dist, draws=kappa, nProtected=n, data={\"a\": 2})\n",
    "\n",
    "#tr = sample_and_eval(plate_chain_dist, draws=kappa, nProtected=2)#, data={\"a\": 2})\n",
    "#tr.trace.out_dicts\n",
    "\n",
    "tr.trace.out_dicts['sample']['__a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index-aware summing\n",
    "\n",
    "We have: one factor corresponding to each variable (latent or observed)\n",
    "\n",
    "e.g. the trace output for the 4 gaussians in our chain example\n",
    "\n",
    "every time we sample, we add a new dimension. Need to delete these after eval\n",
    "\n",
    "### No plate case\n",
    "\n",
    "1. Take all indices `set(I)` in the tensors $T_a$ that depend on `__a` (that have `_k__a` in their names)\n",
    "2. use pytorch names to order the dims the same in each tensor\n",
    "3. multiply $T_a$ (as in `*`)\n",
    "4. sum out `__a`\n",
    "\n",
    "\n",
    "do the reduction as a for-loop (picking the first K dimension and combining all the tensors with that dimension).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plates: []\n",
      "('_K', 'pos_A', 'pos_B')\n",
      "('_k__d', '_k__c', 'pos_A', 'pos_B')\n",
      "tensor([[-18.0145, -32.0846],\n",
      "        [-12.1288, -38.3957]], names=('_k__c', '_k__b'))\n",
      "tensor([-53.0691, -53.9199], names=('_k__c',))\n",
      "tensor([-8.6897, -1.8068], names=('_k__c',))\n",
      "tensor(-18.5551)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_k__c': tensor(-18.5551)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa = 2\n",
    "n = 2\n",
    "data = {} # {\"a\": [4] * 100}\n",
    "tr = sample_and_eval(chain_dist, draws=kappa, nProtected=n, data=data)\n",
    "tensors = tr.trace.out_dicts['log_prob']\n",
    "\n",
    "tpp.combine_tensors(tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing VI\n",
    "\n",
    "Set up a Gaussian graphical model:\n",
    "$$z \\sim N(0,1)$$\n",
    "$$x \\sim N(z, 1)$$\n",
    "Now, we can get $P(x| z) = N(\\mu_{x|z}, \\Sigma_{xx|z})$ analytically \n",
    "\n",
    "$$\\mu_{x|z} = \\mathbf{\\mu_x + \\Sigma_{xz}\\Sigma_{zz}^{-1}(z-\\mu_z)}$$ \n",
    "$$\\Sigma_{x|z} = \\mathbf{\\Sigma_{xx} - \\Sigma_{xz}\\Sigma_{zz}^{-1}\\Sigma_{zx}}$$\n",
    "\n",
    "---\n",
    "\n",
    "We use the approximate posterior, $$Q(z) = N(\\mu, \\sigma^2)$$, where mu and sigma are learned parameters. \n",
    "\n",
    "When we learn using our ELBO-thing, do those parameters learn to match the true posterior?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. bivariate example\n",
    "z_mean, z_var = 0, 1\n",
    "x_var = 1\n",
    "rho = 0.5\n",
    "\n",
    "z = 0 #z.sample([n]).mean()\n",
    "GROUND_TRUTH_POST_MU = u.biv_norm_conditional_mean(z, z_mean, np.sqrt(x_var), \\\n",
    "                                              np.sqrt(z_var), rho, z)\n",
    "GROUND_TRUTH_POST_VAR = u.biv_norm_conditional_var(x_var, rho)\n",
    "\n",
    "GROUND_TRUTH_MU, GROUND_TRUTH_VAR\n",
    "\n",
    "\n",
    "# u.analytical_posterior_var(var, X)\n",
    "# u.analytical_posterior_mean(prior_mean, var, X, Z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI without plates\n",
    "\n",
    "i.e. no repeating bits to abstract over\n",
    "\n",
    "Optimise the params of an approx posterior over extended Z-space, but not K space\n",
    "\n",
    "$$Q (Z|X) = \\prod_k  Q(Z^k|X) = \\prod_k \\prod_i Q(Z^k_i \\mid Z^k_{qa(i)})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\prod_j f_j^{\\kappa_j} = \\frac{P(x_, Z)}{\\prod Q(z_i^{k_i})}$$\n",
    "\n",
    "Writing out the target (log marginal likelihood) fully makes the computation clear:\n",
    "\n",
    "$$ \\mathcal{L}= E_{Q(Z|X)} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "$$= E_{Q} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call sampler on Q. \n",
    "# gives you the samples and a log Q tensor `log_prob`\n",
    "\n",
    "# (implemented as call `sampler` then run the chain_dist)\n",
    "\n",
    "# pass these to evaluator, which does a lookup for all the latents\n",
    "# gives you log P for each latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI with plates\n",
    "\n",
    "$T$: underlying tensor list \n",
    "\n",
    "1. Get plate order\n",
    "    - by looking for the first tensor with that plate\n",
    "    - and using the index of that tensor?\n",
    "2. Sum out plates in reverse order of definition\n",
    "\n",
    "For p in reverse(plates):\n",
    "* $T_{\\mathrm{new}} = []$\n",
    "\n",
    "* $T_p \\leftarrow$ all tensors in p\n",
    "\n",
    "* Remove $T_p$ from $T$\n",
    "\n",
    "* $T_{\\mathrm{new}} \\leftarrow T_p$\n",
    "\n",
    "* Sum out all sample indexes within the plate\n",
    "\n",
    "* $T_{\\backslash p} \\leftarrow$ Sum out the plate\n",
    "\n",
    "* $T += T_{\\backslash p}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_by_plate(tensor_dict) :\n",
    "    \"\"\"\n",
    "    :param tensor_dict: dict of log_prob tensors\n",
    "    :return: dict of dicts of log_prob tensors, dividing by plates\n",
    "    \"\"\"\n",
    "    return NotImplementedError()\n",
    "\n",
    "\n",
    "# Sum:  sum over each plate\n",
    "def plate_sum() :\n",
    "    return NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider Figure 1 from TMC\n",
    "\n",
    "\"\"\"\n",
    "z2 | z1\n",
    "z3 | z1\n",
    "z4 | z2\n",
    "x | z3, z4\n",
    "\"\"\"\n",
    "def simple_dist(trace):\n",
    "    k1 = trace[\"i1\"](WrappedDist(Normal, t.ones(3), 3))\n",
    "    k2 = trace[\"i2\"](WrappedDist(Normal, k1, 3))\n",
    "    k3 = trace[\"i3\"](WrappedDist(Normal, k2, 3))\n",
    "    (k3,) = trace.delete_names((\"i1\", \"i2\"), (k3,))\n",
    "    k4 = trace[\"i4\"](WrappedDist(Normal, k3, 3))\n",
    "    \n",
    "    return k4\n",
    "\n",
    "tr = sample_and_eval(simple_dist, draws=kappa, nProtected=2)#, data={\"a\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index-aware sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
