{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three types of dimension:\n",
    "* plate dims\n",
    "* sample dimensions (usually indexed K)\n",
    "* user dims; underlying dimensions (which the user gets to interact with). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from tpp_trace import *\n",
    "import utils as u\n",
    "import tensor_ops as tpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleech/anaconda3/lib/python3.6/site-packages/torch/tensor.py:723: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:840.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6436, -0.9349,  0.1692]],\n",
       "\n",
       "        [[-1.3581,  5.3876,  1.1136]],\n",
       "\n",
       "        [[ 6.9046,  2.4368,  1.1325]]], names=('_k__a', 'pos_A', 'pos_B'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kappa = 3\n",
    "n = 2\n",
    "tr = sample_and_eval(chain_dist, draws=kappa, nProtected=n, data={\"a\": 2})\n",
    "\n",
    "#tr = sample_and_eval(plate_chain_dist, draws=kappa, nProtected=2)#, data={\"a\": 2})\n",
    "#tr.trace.out_dicts\n",
    "\n",
    "tr.trace.out_dicts['sample']['__a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index-aware summing\n",
    "\n",
    "We have: one factor corresponding to each variable (latent or observed)\n",
    "\n",
    "e.g. the trace output for the 4 gaussians in our chain example\n",
    "\n",
    "every time we sample, we add a new dimension. Need to delete these after eval\n",
    "\n",
    "### No plate case\n",
    "\n",
    "1. Take all indices `set(I)` in the tensors $T_a$ that depend on `__a` (that have `_k__a` in their names)\n",
    "2. use pytorch names to order the dims the same in each tensor\n",
    "3. multiply $T_a$ (as in `*`)\n",
    "4. sum out `__a`\n",
    "\n",
    "\n",
    "do the reduction as a for-loop (picking the first K dimension and combining all the tensors with that dimension).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_values([tensor(7.4262)]),\n",
       " dict_values([tensor(-5.4262)]),\n",
       " tensor(-10.6475))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(tpp)\n",
    "imp.reload(u)\n",
    "\n",
    "kappa = 2\n",
    "n = 2\n",
    "data = {} # {\"a\": [4] * 100}\n",
    "tr = sample_and_eval(chain_dist, draws=kappa, nProtected=n, data=data)\n",
    "tensors = tr.trace.out_dicts['log_prob']\n",
    "\n",
    "X = t.Tensor([[.3,.1],\\\n",
    "              [.1,.3]])\n",
    "X = X.refine_names('_k__a', '_k__b')\n",
    "Y = t.Tensor([[.3,.7],\\\n",
    "              [.2,.3]])\n",
    "Y = Y.refine_names('_k__a', '_k__b')\n",
    "tensors = {'__a' : X.log(), '__b' : Y.log() } \n",
    "\n",
    "tpp.combine_tensors(tensors, naive=True).values(), \\\n",
    "tpp.combine_tensors(tensors, naive=False).values(), \\\n",
    "u.logmmmeanexp(tensors[\"__a\"], tensors[\"__b\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3612, 0.3512]) \n",
      " tensor([0.3612, 0.3512])\n"
     ]
    }
   ],
   "source": [
    "def named_example_2D() :\n",
    "    X = t.Tensor([[.2,.1],\\\n",
    "                  [.1,.3]])\n",
    "    X = X.refine_names('_k__a', '_k__b')\n",
    "    Y = t.Tensor([[.3,.2],\\\n",
    "                  [.1,.1]])\n",
    "    Y = Y.refine_names('_k__a', '_k__b')\n",
    "    \n",
    "    return X, Y, '_k__a'\n",
    "\n",
    "\n",
    "def test_logmulmeanexp() :\n",
    "    X, Y, dim = named_example_2D()\n",
    "    log_mean_prod_exp = u.logmulmeanexp(X, Y, dim)\n",
    "    \n",
    "    reference = u.logmmmeanexp(X, Y) \\\n",
    "                .rename(None) \\\n",
    "                .diag()\n",
    "    stripped = log_mean_prod_exp \\\n",
    "                .rename(None) \\\n",
    "                .squeeze()\n",
    "    \n",
    "    #assert(t.allclose(stripped, reference) )\n",
    "    print(reference, \"\\n\", stripped)\n",
    "    \n",
    "test_logmulmeanexp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing VI\n",
    "\n",
    "Set up a Gaussian graphical model:\n",
    "$$z \\sim N(0,1)$$\n",
    "$$x \\sim N(z, 1)$$\n",
    "Now, we can get $P(x| z) = N(\\mu_{x|z}, \\Sigma_{xx|z})$ analytically \n",
    "\n",
    "$$\\mu_{x|z} = \\mathbf{\\mu_x + \\Sigma_{xz}\\Sigma_{zz}^{-1}(z-\\mu_z)}$$ \n",
    "$$\\Sigma_{x|z} = \\mathbf{\\Sigma_{xx} - \\Sigma_{xz}\\Sigma_{zz}^{-1}\\Sigma_{zx}}$$\n",
    "\n",
    "---\n",
    "\n",
    "We use the approximate posterior, $$Q(z) = N(\\mu, \\sigma^2)$$, where mu and sigma are learned parameters. \n",
    "\n",
    "When we learn using our ELBO-thing, do those parameters learn to match the true posterior?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g. bivariate example\n",
    "z_mean, z_var = 0, 1\n",
    "x_var = 1\n",
    "rho = 0.5\n",
    "\n",
    "z = 0 #z.sample([n]).mean()\n",
    "GROUND_TRUTH_POST_MU = u.biv_norm_conditional_mean(z, z_mean, np.sqrt(x_var), \\\n",
    "                                              np.sqrt(z_var), rho, z)\n",
    "GROUND_TRUTH_POST_VAR = u.biv_norm_conditional_var(x_var, rho)\n",
    "\n",
    "GROUND_TRUTH_POST_MU, GROUND_TRUTH_POST_VAR\n",
    "\n",
    "\n",
    "# u.analytical_posterior_var(var, X)\n",
    "# u.analytical_posterior_mean(prior_mean, var, X, Z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI without plates\n",
    "\n",
    "i.e. no repeating bits to abstract over\n",
    "\n",
    "Optimise the params of an approx posterior over extended Z-space, but not K space\n",
    "\n",
    "$$Q (Z|X) = \\prod_k  Q(Z^k|X) = \\prod_k \\prod_i Q(Z^k_i \\mid Z^k_{qa(i)})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\prod_j f_j^{\\kappa_j} = \\frac{P(x_, Z)}{\\prod Q(z_i^{k_i})}$$\n",
    "\n",
    "Writing out the target (log marginal likelihood) fully makes the computation clear:\n",
    "\n",
    "$$ \\mathcal{L}= E_{Q(Z|X)} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "$$= E_{Q} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TMC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cffd4b6ec8a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://github.com/anonymous-78913/tmc-anon/blob/master/non-fac/model.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTMC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrsample_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mKs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mKs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TMC' is not defined"
     ]
    }
   ],
   "source": [
    "# https://github.com/anonymous-78913/tmc-anon/blob/master/non-fac/model.py\n",
    "class Fac(TMC):\n",
    "    def rsample_log_prob(self, Ks):\n",
    "        if isinstance(Ks, int):\n",
    "            Ks = self.T.item() * [Ks]\n",
    "\n",
    "        zs = []\n",
    "        log_probs = []\n",
    "\n",
    "        for i in range(len(Ks)):\n",
    "            Q = Normal(self.means[i], self.log_stds[i].exp())\n",
    "            z = Q.rsample(sample_shape=t.Size([Ks[i]]))\n",
    "            zs.append(z)\n",
    "            log_probs.append(Q.log_prob(z))\n",
    "\n",
    "        return zs, log_probs\n",
    "    \n",
    "    \n",
    "class TMC(nn.Module):\n",
    "    def __init__(self, T, res, log_std=0., like_std=1.):\n",
    "        super().__init__()\n",
    "        self.register_buffer('T', t.tensor(T))\n",
    "        self.register_buffer('res', t.tensor(res))\n",
    "        #self.res      = res\n",
    "        self.means    = nn.Parameter(t.zeros(T))\n",
    "        self.log_stds = nn.Parameter(t.sqrt((1.+t.arange(T).float())/T))#log_std*t.ones(T))\n",
    "        self.like_std = like_std\n",
    "\n",
    "    def logpqs(self, Ks):\n",
    "        \"\"\"\n",
    "        Compute the series of tensors that we reduce over, by combining the universal generative model\n",
    "        with the proposal propbabilities.\n",
    "        \"\"\"\n",
    "        zs, log_qs = self.rsample_log_prob(Ks)\n",
    "\n",
    "        log_ps = Normal(0., t.sqrt(1./self.T.float())).log_prob(zs[0])\n",
    "        res = [(log_ps - log_qs[0]).unsqueeze(0)]\n",
    "\n",
    "        for i in range(1, len(zs)):\n",
    "            log_ps = Normal(zs[i-1].unsqueeze(1), t.sqrt(1/self.T.float())).log_prob(zs[i])\n",
    "            res.append(log_ps - log_qs[i])\n",
    "\n",
    "        res.append(Normal(zs[-1].unsqueeze(1), self.like_std).log_prob(self.res))\n",
    "\n",
    "        return res\n",
    "\n",
    "    def reduce(self, Ks):\n",
    "        \"\"\"\n",
    "        Combine tensors\n",
    "        \"\"\"\n",
    "        logpqs = self.logpqs(Ks)\n",
    "\n",
    "        res = logpqs[0]\n",
    "        for i in range(1, len(logpqs)):\n",
    "            res = logmmmeanexp(res, logpqs[i])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorisedDist(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a list of distributions and allows e.g. sampling from them \n",
    "    \"\"\"\n",
    "    def __init__(self, dists):\n",
    "        super().__init__()\n",
    "        self.dists = dists\n",
    "\n",
    "    def rsample(self):\n",
    "        return [d.rsample() for d in self.dists]\n",
    "\n",
    "    def log_prob(self, zs):\n",
    "        return [d.log_prob(z) for (d, z) in zip(self.dists, zs)]\n",
    "\n",
    "    def rsample_log_prob(self):\n",
    "        zs = self.rsample()\n",
    "        lps = self.log_prob(zs)\n",
    "        return zs, lps\n",
    "    \n",
    "    \n",
    "class FacRandomSequential(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Transforms input into a series of factorised distributions\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        mods = list(self._modules.values())\n",
    "        z = input\n",
    "        dists = []\n",
    "        for i in range(len(mods)):\n",
    "            z, dist = mods[i](z)\n",
    "            dists.append(dist)\n",
    "        \n",
    "        return FactorisedDist(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Usual single/multi-sample VAE\n",
    "    \"\"\"\n",
    "    def __init__(self, p, q, K):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, x):\n",
    "        wz = self.q.sample(x.size(0), sample_shape=t.Size([self.K]))\n",
    "        elbo = self.p.log_prob((x, wz)) - self.q.log_prob(wz)\n",
    "        lme = logmeanexp(elbo)\n",
    "        return lme\n",
    "\n",
    "    def train(self, x):\n",
    "        opt = t.optim.Adam(q.parameters())\n",
    "        for i in range(100):\n",
    "            opt.zero_grad()\n",
    "            obj = self(x)\n",
    "            (-obj).backward()\n",
    "            opt.step()\n",
    "            print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI with plates\n",
    "\n",
    "$T$: underlying tensor list \n",
    "\n",
    "1. Get plate order\n",
    "    - by looking for the first tensor with that plate\n",
    "    - and using the index of that tensor?\n",
    "2. Sum out plates in reverse order of definition\n",
    "\n",
    "For p in reverse(plates):\n",
    "* $T_{\\mathrm{new}} = []$\n",
    "\n",
    "* $T_p \\leftarrow$ all tensors in p\n",
    "\n",
    "* Remove $T_p$ from $T$\n",
    "\n",
    "* $T_{\\mathrm{new}} \\leftarrow T_p$\n",
    "\n",
    "* Sum out all sample indexes within the plate\n",
    "\n",
    "* $T_{\\backslash p} \\leftarrow$ Sum out the plate\n",
    "\n",
    "* $T += T_{\\backslash p}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_by_plate(tensor_dict) :\n",
    "    \"\"\"\n",
    "    :param tensor_dict: dict of log_prob tensors\n",
    "    :return: dict of dicts of log_prob tensors, dividing by plates\n",
    "    \"\"\"\n",
    "    return NotImplementedError()\n",
    "\n",
    "\n",
    "# Sum:  sum over each plate\n",
    "def plate_sum() :\n",
    "    return NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider Figure 1 from TMC\n",
    "\n",
    "\"\"\"\n",
    "z2 | z1\n",
    "z3 | z1\n",
    "z4 | z2\n",
    "x | z3, z4\n",
    "\"\"\"\n",
    "def simple_dist(trace):\n",
    "    k1 = trace[\"i1\"](WrappedDist(Normal, t.ones(3), 3))\n",
    "    k2 = trace[\"i2\"](WrappedDist(Normal, k1, 3))\n",
    "    k3 = trace[\"i3\"](WrappedDist(Normal, k2, 3))\n",
    "    (k3,) = trace.delete_names((\"i1\", \"i2\"), (k3,))\n",
    "    k4 = trace[\"i4\"](WrappedDist(Normal, k3, 3))\n",
    "    \n",
    "    return k4\n",
    "\n",
    "tr = sample_and_eval(simple_dist, draws=kappa, nProtected=2)#, data={\"a\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index-aware sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
