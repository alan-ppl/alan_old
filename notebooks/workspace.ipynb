{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import random\n",
    "import torch as t\n",
    "from torch.distributions import Normal\n",
    "from functools import reduce\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from tpp_trace import *\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example directed graph without plate repeats\n",
    "# the real stuff happens as side effects in trace's dicts\n",
    "def chain_dist(trace):\n",
    "    a = trace[\"a\"](WrappedDist(Normal, t.ones(3), 3))\n",
    "    b = trace[\"b\"](WrappedDist(Normal, a, 3))\n",
    "    c = trace[\"c\"](WrappedDist(Normal, b, 3))\n",
    "    (c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "    d = trace[\"d\"](WrappedDist(Normal, c, 3))\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "# example directed graph with plate repeats\n",
    "# 3(a) -> 4(b) -> c -> d\n",
    "def plate_chain_dist(trace):\n",
    "    a = trace[\"a\"](WrappedDist(Normal, t.ones(3), 3), plate_name=\"A\", plate_shape=3)\n",
    "    b = trace[\"b\"](WrappedDist(Normal, a, 3),         plate_name=\"B\", plate_shape=4)\n",
    "    c = trace[\"c\"](WrappedDist(Normal, b, 3))\n",
    "    (c,) = trace.delete_names((\"a\", \"b\"), (c,))\n",
    "    d = trace[\"d\"](WrappedDist(Normal, c, 3))\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def sampler(draws, nProtected, data={}):\n",
    "    return trace({\"data\":data}, \\\n",
    "                 SampleLogProbK(K=draws, protected_dims=nProtected) \\\n",
    "                )\n",
    "\n",
    "def evaluator(sampler, nProtected, data={}) :\n",
    "    tr = sampler.trace\n",
    "    d = {\"data\": data, \"sample\": tr.out_dicts[\"sample\"]} # error if called before sampler\n",
    "    lpk = LogProbK(plate_names=tr.fn.plate_names, protected_dims=nProtected)\n",
    "    \n",
    "    return trace(d, lpk)\n",
    "\n",
    "\n",
    "def sample_and_eval(model, draws, nProtected, data={}) :\n",
    "    tr1 = sampler(draws, nProtected, data=data)\n",
    "    val = model(tr1)\n",
    "    print(\"Plates:\", tr1.trace.fn.plate_names)\n",
    "    print(val.names)\n",
    "    tr2 = evaluator(tr1, nProtected, data=data)\n",
    "    val = model(tr2)\n",
    "    print(val.names)\n",
    "    \n",
    "    return tr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plates: []\n",
      "('_K', 'pos_A', 'pos_B')\n",
      "('_k__d', '_k__c', 'pos_A', 'pos_B')\n",
      "Plates: ['_plate_A', '_plate_B']\n",
      "('_plate_B', '_plate_A', '_K', 'pos_A', 'pos_B')\n",
      "('_k__d', '_k__c', '_plate_B', '_plate_A', 'pos_A', 'pos_B')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleech/anaconda3/lib/python3.6/site-packages/torch/tensor.py:723: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:840.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    }
   ],
   "source": [
    "kappa = 3\n",
    "\n",
    "tr = sample_and_eval(chain_dist, draws=kappa, nProtected=2)#, data={\"a\": 2})\n",
    "#tr.trace.out_dicts\n",
    "\n",
    "tr = sample_and_eval(plate_chain_dist, draws=kappa, nProtected=2)#, data={\"a\": 2})\n",
    "#tr.trace.out_dicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index-aware summing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have: one factor corresponding to each variable (latent or observed)\n",
    "# e.g. the trace output for the 4 gaussians in our example\n",
    "\n",
    "# Combine: \n",
    "# list of log_prob tensors -> list of log_prob tensors, summed across groups\n",
    "# TODO: add data handling\n",
    "# TODO: add plates\n",
    "def combine_tensors(tensors) :\n",
    "    names = list(tensors.keys()) # variable names\n",
    "    tensors = tensors.values()\n",
    "    print(names)\n",
    "    \n",
    "    while len(names) > 2:\n",
    "        # choose a random index (e.g. \"a\")\n",
    "        i = names.pop(random.randrange(len(names)))\n",
    "\n",
    "        # remove all factors which depend on \"a\"\n",
    "        tensors, i_tensors = get_dependent_factors(tensors, i)\n",
    "        \n",
    "        #Take the tensor product of all these factors 𝑡_a = ∏ 𝑡|𝐾_a\n",
    "        T = reduce(t.matmul, i_tensors)\n",
    "        # need a `view` here?\n",
    "        # Is opt_einsum necessary for controlling dim explosion?\n",
    "        # squeeze?\n",
    "        \n",
    "        #Sum out 𝑘^a  for that variable →𝑡∖a\n",
    "        T = T.sum(i)\n",
    "        #TODO: plate_sum()\n",
    "        \n",
    "        # Put 𝑡∖a back in the list\n",
    "        tensors.append(T)\n",
    "        \n",
    "        # TODO: when does the split by group happen?\n",
    "        # groupby(tensors)\n",
    "        \n",
    "    # Then the last two tensors are simple\n",
    "    #T = combine_two_tensors(tensors)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return T\n",
    "\n",
    "\n",
    "# factors T∣K_i that depend on K_i\n",
    "def get_dependent_factors(tensors, i):\n",
    "    i_dim = k_dim_name(i)\n",
    "    dependents = [tensor for tensor in tensors \\\n",
    "                    if i_dim in tensor.names]\n",
    "    nondependents = [tensor for tensor in tensors \\\n",
    "                        if i_dim not in tensor.names]\n",
    "    \n",
    "    return nondependents, dependents\n",
    "\n",
    "\n",
    "def combine_two_tensors(tensors) :\n",
    "    assert len(tensors) == 2\n",
    "    return u.logmmmeanexp(*tensors)\n",
    "\n",
    "# groups \n",
    "# i.e. split the arg_names into indices and latents\n",
    "# (otherwise, we will conflate _k_dims and latent variables)\n",
    "def groupby(tensors) :\n",
    "    return NotImplementedError()\n",
    "\n",
    "\n",
    "# Rearrange:  \n",
    "# dict of log_prob tensors -> dict of dicts of log_prob tensors, dividing by plates\n",
    "def rearrange_by_plate(tensor_dict) :\n",
    "    return NotImplementedError()\n",
    "\n",
    "\n",
    "# Sum:  sum over each plate\n",
    "def plate_sum() :\n",
    "    return NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plates: []\n",
      "('_K', 'pos_A', 'pos_B')\n",
      "('_k__d', '_k__c', 'pos_A', 'pos_B')\n",
      "['__a', '__b', '__c', '__d']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size 3 at dimension 1, but got size 1 for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-83bd548457cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnProtected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, data={\"a\": 2})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcombine_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_prob\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-c0b1b90d6776>\u001b[0m in \u001b[0;36mcombine_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#Take the tensor product of all these factors 𝑡_a = ∏ 𝑡|𝐾_a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# need a `view` here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Is opt_einsum necessary for controlling dim explosion?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size 3 at dimension 1, but got size 1 for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "kappa = 2\n",
    "\n",
    "tr = sample_and_eval(chain_dist, draws=kappa, nProtected=2)#, data={\"a\": 2})\n",
    "combine_tensors(tr.trace.out_dicts[\"log_prob\"])\n",
    "\n",
    "\n",
    "tr.trace.out_dicts[\"log_prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI without plates\n",
    "\n",
    "i.e. no repeating bits to abstract over\n",
    "\n",
    "$$Q (Z|X) = \\prod_k  Q(Z^k|X) = \\prod_k \\prod_i Q(Z^k_i \\mid Z^k_{qa(i)})$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\prod_j f_j^{\\kappa_j} = \\frac{P(x_, Z)}{\\prod Q(z_i^{k_i})}$$\n",
    "\n",
    "Writing out the target (log marginal likelihood) fully makes the computation clear:\n",
    "\n",
    "$$ \\mathcal{L}= E_{Q(Z|X)} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$\n",
    "$$= E_{Q} \\left[ \\log \\frac{∑_K  P(Z,K,X)}{Q (Z|X)} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get factors \n",
    "2. sum out indices until you get a scalar log prob\n",
    "3. gradient ascent\n",
    "\n",
    "\n",
    "### 2. the sum (combine)\n",
    "\n",
    "a. until the last two, do clever thing\n",
    "\n",
    "b. and we don't need to be clever when there's only two tensors\n",
    "\n",
    "### The clever thing\n",
    "\n",
    "1. choose a random index (e.g. \"a\")\n",
    "2. take all vectors which use \"a\"\n",
    "3. matmul + flatten\n",
    "    - this gives you a giant tensor so be clever (maybe `opt.einsum`, but probably not)\n",
    "4. sum out \"a\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum out $K_5$\n",
    "\n",
    "1. Remove all tensors / factors $T \\mid K_5$ from the list that depend on $K_5$\n",
    "2. Take the tensor product of all these factors $t_5 = \\prod t |K_5$\n",
    "3. Sum out $k_i^{(5)}$ for that variable $\\to t_{\\backslash 5}$\n",
    "4. Put $t_{\\backslash 5}$ back in the list\n",
    "\n",
    "# 3.\n",
    "a. \"sum(I)\" =  we  multiply  all  factors  in  the  list  that  depend  on I, \n",
    "\n",
    "b. sum I out, \n",
    "\n",
    "c. and put the result back in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test case: we want to compute the true posterior, especially its variance\n",
    "\n",
    "def P(trace):\n",
    "    a = WrappedDist(Normal, t.ones(2), 3)\n",
    "    b = WrappedDist(Normal, t.ones(2), 3)\n",
    "    #return (a+b)\n",
    "\n",
    "#class Q(nn.Module):\n",
    "#    def __init__(self)\n",
    "\n",
    "\n",
    "tr1 = trace({\"data\": {}}, SampleLogProbK(K=4, protected_dims=2))\n",
    "P(tr1)\n",
    "\n",
    "# needs a prior and approximate posterior\n",
    "# Lookup Gaussian conditioning in Bishop for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI with plates\n",
    "\n",
    "$T$: underlying tensor list \n",
    "\n",
    "1. Get plate order\n",
    "    - by looking for the first tensor with that plate\n",
    "    - and using the index of that tensor?\n",
    "2. Sum out plates in reverse order of definition\n",
    "\n",
    "For p in reverse(plates):\n",
    "* $T_{\\mathrm{new}} = []$\n",
    "\n",
    "* $T_p \\leftarrow$ all tensors in p\n",
    "\n",
    "* Remove $T_p$ from $T$\n",
    "\n",
    "* $T_{\\mathrm{new}} \\leftarrow T_p$\n",
    "\n",
    "* Sum out all sample indexes within the plate\n",
    "\n",
    "* $T_{\\backslash p} \\leftarrow$ Sum out the plate\n",
    "\n",
    "* $T += T_{\\backslash p}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index-aware sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laurence's example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('_K', 'pos_A', 'pos_B')\n",
      "('_k__d', '_k__c', 'pos_A', 'pos_B')\n"
     ]
    }
   ],
   "source": [
    "nProtected = 2\n",
    "draws = 4 # kappa\n",
    "\n",
    "tr1 = trace({\"data\": {}}, SampleLogProbK(K=draws, protected_dims=nProtected))\n",
    "val = chain_dist(tr1)\n",
    "print(val.names)\n",
    "tr2 = trace({\"data\": {}, \"sample\": tr1.trace.out_dicts[\"sample\"]}, \\\n",
    "            LogProbK(tr1.trace.fn.plate_names, nProtected))\n",
    "val = chain_dist(tr2)\n",
    "print(val.names)\n",
    "#tr2.trace.out_dicts#[\"sample\"]#[\"__a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5.],\n",
       "        [5., 5.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = t.ones(2,4) *3\n",
    "Y = t.ones(4,2) *2\n",
    "u.logmmmeanexp(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
